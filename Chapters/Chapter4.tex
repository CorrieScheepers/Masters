\chapter{Conclusion}
\label{chap:4}
\ChapterPageStuff{4}

\section{Discussion}

\subsection{Contributions made to literature}
\Cref{chap:1} highlighted the importance of software maintenance throughout the life cycle of most software systems. A gap in the literature was identified in \Cref{sec:ch1_gapIdentification} from the studies obtained on software maintenance. \par Using event logging to create a log analysis for utilising the software systems is a possible solution to make software maintenance prioritisations. A state of the art analysis was performed for the literature on software maintenance, event logging, and log analysis and outlined in \Cref{tbl:ch1_stateOfTheArt2}. This study will be referred to as \textit{Study U}, filling in the gaps in the literature obtained with the method in \Cref{chap:2} that is included in the updated state of the art in \Cref{tbl:ch4_stateOfTheArt}.

\begin{table}[!htb]
	\centering
	\caption[State of the art]
	{\textit{State of the art}}
	\label{tbl:ch4_stateOfTheArt}
	\begin{threeparttable}
		\begin{tabularx}{\textwidth}{cYYYYYY}
			\toprule
			\thead{Ref.} & \multicolumn{3}{c}{\thead{Software maintenance}} & \multicolumn{2}{c}{\thead{Event logging}} & \multicolumn{1}{c}{\thead{Log analysis}} \\ 
			 \cmidrule(lr){2-4} \cmidrule(lr){5-6} \cmidrule(lr){7-7}
			 & \RaggedRight \thead{Models} & \RaggedRight \thead{Problems} & \RaggedRight \thead{Prioritisation} & \thead{Parsing} & \thead{Points} & \RaggedRight \thead{Utilisation} \\ 		
			\midrule
				\sotaCore
				\rowcolor{pastelgreen}
				\textit{U}\tnote{$\ast$} & Partial & \cmark & \cmark & \cmark & \cmark & \cmark \\
			\bottomrule
		\end{tabularx}
		\begin{tablenotes}\footnotesize
			\item[$\ast$] \textit{Study U:~\ThesisTitle}
		\end{tablenotes}
	\end{threeparttable}
\end{table}

\Cref{tbl:ch4_stateOfTheArt} highlights a divide between software maintenance, event logging, and log analysis in the state of the art. \textit{Study U} can contribute to the literature by providing a comprehensive method that includes all three main research topics. \par Event logging and log analysis as a primary topic overlap in the literature more often than software maintenance. Research on software maintenance was identified as being limited by a lack of third-party validation, not always trying to improve or build existing software maintenance models, little comparison with other literature, and difficulty replicating due to private data sets or custom tools. \par The obtained studies that discussed software maintenance frequently identified that the efficiency of implementing software maintenance is essential. Some studies attempted to use specific software maintenance models on certain software maintenance types for better efficiency. \par Various studies tried to create a task distribution for developers to improve software maintenance efficiency. Other studies attempted to isolate certain software maintenance resource costs in the entire Software Development Life-Cycle (SDLC). \par Additionally, these studies did not attempt to provide the method needed to prioritise these software systems. The method requires a logging mechanism to do the log analysis, which is not present in software maintenance-related studies. \par \textit{Study U} does not explicitly use software maintenance models as it is irrelevant to prioritising software systems. For this reason, the model's state of the art topic is marked as partially achieved. From the software maintenance problems identified in the literature, the requirements for the log analysis are created for the software maintenance prioritisation. \par The methods for the event logging mechanisms are not fully described or use third-party tools. Software maintenance does not overlap with this topic, as most event logs are for system diagnostics. \par There were logging mechanisms that captured user-based logs used for user-behavioural analysis instead of system utilisation analysis. \textit{Study U} contributes to the literature on system utilisation analysis for software maintenance when prioritised.

\subsection{Value added to industry}
The method in \Cref{chap:2} is used in three different case studies in \Cref{sec:ch3_caseStudies}. Each of these case studies explored the various applications of the generic logging mechanism to obtain user-based events. The results proved that the logging mechanism could get the desired user-based activities. However, additional adaptations were needed for each case study to ensure that log quality was acceptable for consistent and reliable log analysis. \par These adaptations were due to the software environment (software languages and design methodologies used) and the purpose of the software system. Older systems require different logging points to produce the same result as newer software systems that use less or one logging point. \par These systems used the same user-based event type with similar operational software use cases. Systems with the same software architecture but different functional use cases also had various adaptations to their logging mechanism. These adaptations ensured that the desired log attributes were correctly obtained for the log analysis. \par The maintenance prioritisation recommendations for each case study used the defined generic methodology. The log analysis for each case study was similar to the log requirements for the user-based utilisation for all the case studies and were only different for each of their user-based event types. These user-based event types represent the operational use cases for the case study to observe the kind of utilisation of the subsystems further. \par The results proved that the generic methodology defined in \Cref{chap:2} could be implemented on different software systems with alternative operational use cases for each case study. In the web development industry, software systems similar to the case studies and different software systems will benefit from implementing this methodology to improve software maintenance decisions using these recommendations for prioritisation.

\subsection{Validation strategy}
The following five-step validation strategy is used for the outcomes of this study:

\begin{enumerate}[label=\textbf{\Roman*.}]
	\item Revisit the literature gap defined in \Cref{sec:ch1_gapIdentification}.
	\item Revisit the original problem statement defined in \Cref{sec:ch1_problemStatement}.
	\item Revisit the study objectives defined in \Cref{sec:ch1_objectives}.
	\item Reflect on the results of the study methodology in \Cref{chap:3}.
	\item Determine whether a solution to the original problem statement has been presented. 
\end{enumerate}

This study is validated by:

\begin{enumerate}[label=\textbf{\Roman*.}]
	\item \textbf{Gap in literature} \par The literature gap defined in \Cref{sec:ch1_gapIdentification} for implementing software maintenance is summarised as: 
		\begin{center}
			\begin{tcolorbox}[colback=lightgray, colframe=black, sharp corners=all, arc=4pt]
				\begin{minipage}{\textwidth}
					\RaggedRight\textit{\studyGap}
				\end{minipage}
			\end{tcolorbox}
		\end{center}

	\item \textbf{Original problem statement} \par The original problem statement defined in \Cref{sec:ch1_problemStatement} to make software maintenance prioritisations:
		\begin{center}
			\begin{tcolorbox}[colback=lightgray, colframe=black, sharp corners=all, arc=4pt]
				\begin{minipage}{\textwidth}
					\RaggedRight\textit{\problemStatement}
				\end{minipage}
			\end{tcolorbox}
		\end{center}

	\item \textbf{Study objectives} \par The study objectives for this study that were defined in \Cref{sec:ch1_objectives} and have been met are outlined \Cref{tbl:ch4_ValidationStart}. 

		\begin{xltabular}{\textwidth}{cXp{3cm}c}
			\caption[Study validation]
			{\textit{Study validation}}
			\label{tbl:ch4_ValidationStart} \\

			\toprule
			\thead{Objective ID} & \thead{Objective}  & \thead{Section} & \thead{Objective met} \\ 
			\midrule
			\endfirsthead

			\caption[]{\continueCaption} \\
			\toprule
			\thead{Objective ID} & \thead{Objective}  & \thead{Section} & \thead{Objective met} \\
			\midrule
			\endhead

			\midrule
			\multicolumn{4}{r}{\continueText} \\ \midrule
			\endfoot
			\endlastfoot

			\multicolumn{4}{c}{\thead{Literature Objectives}} \\ 
			\midrule
			\rowcolor{lightgray}
			L1 & \RaggedRight \objAi & \RaggedRight \Cref{sec:ch2_logAttributesRequirements,sec:ch2_webApplicationArchitecture} & \cmark \\
			L2 & \RaggedRight \objAii & \RaggedRight \Cref{sec:ch2_loggingPoints,sec:ch2_webApplicationArchitecture} & \cmark \\
			\rowcolor{lightgray}
			L3 & \RaggedRight \objAiii & \Cref{sec:ch2_logAnalysisTools} & \cmark \\ 
			L4 & \RaggedRight \objAiv & \Cref{sec:ch2_utilisationImprovements} & \cmark \\ 
			\midrule
			\multicolumn{4}{c}{\thead{Empirical Objectives}} \\ 
			\midrule
			\rowcolor{lightgray}
			E1 & \RaggedRight \objBi: 
				\begin{enumerate}
					\item \objBiSubA
					\item \objBiSubB
					\item \objBiSubC
					\item \objBiSubD
				\end{enumerate} & \Cref{sec:ch3_implementation} & \cmark \\
			E2 & \RaggedRight \objBii & \Cref{sec:ch3_Verification} & \cmark \\
			\rowcolor{lightgray}
			E3 & \RaggedRight \objBiii & \Cref{sec:ch3_caseStudies} & \cmark \\
			\bottomrule
		\end{xltabular}

    In \Cref{tbl:ch4_ValidationStart}, for objective L1, the characteristics of the event log are defined with the expected log attributes needed for the log analysis. The characteristics of the event log focus only on obtaining user-based event logs with the necessary qualities that the logging point can capture and store. \par For objective L2, the log attributes of each user-based event must be obtained from the software system using a logging point. The logging points are placed strategically in the software system where they have little impact on the performance of the software system. The requirements for the event logs are created to guide developers in making the logging points to capture the event logs. \par For objective L3, the quality of the logs is essential for log analysis. This objective ensures that the captured event logs are complete, accurate, and available when the logs are extracted by the log analysis tool. The method requirements were defined to adhere to the log quality specifications. \par For objective L4, maintenance prioritisation is determined. The calculations needed for this log analysis are explained for this objective that needs to be executed in the log analysis. Prioritisation of each subsystem will be calculated in the specified time frame. For this objective, the requirements for which logs to use are also defined, as some types of user activity can be excluded based on the software system used. \par Empirical objectives use the method in a test system to verify the results and implement the method in the case studies. It also reflects on the study methodology using the results to determine whether this solution has resolved the problem statement. \par For objective E1, the following sub-objectives are met for the implementation of the method on the test system:

	\begin{enumerate}
		\item The user activity types and log attributes for the test system were defined. A logging point(s) was placed on the client or server side for the test system.
		\item This strategic placement of the logging point(s) was determined by what log attributes need to be obtained in certain stages of the software system's logging mechanism, structure, and complexity. The logging points store the event logs in a structured database.
		\item A log analysis tool is either created or a suitable third-party tool is used to analyse the extracted logs. Log quality is also checked in this objective to determine whether the logs meet the standard set by the requirements in L3. 
		\item Maintenance prioritisation is calculated for each subsystem for this sub-objective using L4.
	\end{enumerate} 

	For objective E2, the results of implementing the method in the test system are verified in this objective. Objectives L1, L2, L3, and L4 verified if the technique could create software maintenance prioritisations for the test system by comparing it to the expected results. \par For objective E3, the verified method of E1 is applied to multiple case studies and the results are evaluated. A critical analysis of the case studies is performed to validate the software maintenance prioritisation for this study, utilising the case studies' results. With this validation strategy, it is validated that this study meets the study objectives with the created solution for the original problem statement.

\item \textbf{Reflection on methodology} \par In \Cref{chap:2}, the development of the functional requirements of the solution is made in \Cref{sec:ch2_developementOfSolution}. This provides a generic method to create a logging mechanism and analyse the obtained user-based logs. A test logging mechanism is designed with a log analysis of usage to validate the development of functional requirements for the solution in \Cref{tbl:ch2_developmenetRequirements}. \par This logging mechanism aims to implement each sub-functional requirement in a test system by testing specific inputs to verify expected outputs. The following validation method was used for this test implementation in \Cref{sec:ch3_implementation}:
	\begin{itemize}
		\item Identifying and creating the log attributes needed for the log analysis (\ref{fr:logAttributes}). The software system's key log attributes are precisely defined for maintenance prioritisation. The user-activity type forms the base requirement for a user-based log.
		
		\item Logging points are made to capture these logging attributes at specific locations in the software system (\ref{fr:loggingPoints}). The placement of the logging points to capture user-based logs is verified if it can be done consistently, accurately, and discretely without impacting the software system's performance. 
		
		\item Log analysis is verified using a third-party log analysis tool or the implementation of a log analysis (\ref{fr:logAnalysis}). For the log analysis to be successful, the quality must be acceptable. This is verified by the logging points' performance and the usability of the logs without too many post-logging corrections made.
		
		\item Creating software maintenance prioritising (\ref{fr:maintenancePrioritising}) from the results of log analysis. In the log analysis, the different subsystems' maintenance priority ($M_{PF}$) are calculated from the normalised total active users ($P_N$) multiplied by the normalised total user activity ($A_N$) for a specified subsystem in \Cref{eq:ch2_priorityNormalised,eq:ch2_maintenanceFactorSimplified,eq:ch2_eventNormalised}. \par These results are verified with the test case study and Case Studies A, B, and C in \Cref{sec:ch3_caseStudies} on different software systems with different operational use cases. The results obtained for the maintenance priority validate the previous implementation using the defined user-based logs to perform the log analysis for the maintenance priority. 
	\end{itemize}

	\clearpage

\item \textbf{Conclusion} \par The following outcomes on how the solution has been presented to solve the original problem statement for this study are presented:
	\begin{itemize}
		\item The solution in this study aims to prioritise software maintenance by using event logging and log analysis.
		\item The study results showed that the solution could create a logging mechanism for prioritisation of software maintenance, as stipulated in the study objectives of \Cref{tbl:ch4_ValidationStart}.
		\item Therefore, the need to develop a method for log analysis for software maintenance by creating a suitable logging mechanism to capture user-based event logs has been addressed by the study objectives.
		\item The original problem statement defined in \Cref{sec:ch1_problemStatement} has been successfully addressed by the study objectives.
		\item Finally, the identified gap has been addressed and fulfilled by providing a solution to the original problem statement.
	\end{itemize}
\end{enumerate}

\section{Recommendations}
In \Cref{sec:ch3_caseStudies}, the case studies highlighted the method's limitations.

\subsection{Automated logging and AI tools}
In \Cref{sec:ch1_AI}, automated logging using AI (artificial intelligence) was stated as not being part of the scope of this research due to the reasons listed in the section. However, using AI tools could potentially address the research problem and enhance the creation of a suitable log mechanism and analysis. \par Future research should delve into the development of user activity logging solutions that leverage AI tools for automated logging. AI tools can further integrate machine learning techniques to automatically analyse and derive meaningful patterns from large-scale user activity logs. This will provide valuable insights and predictions for the maintenance priority. 

\subsection{Logging quality}
\Cref{sec:ch1_loggingQuality} describes the event log quality as an essential component of the logging mechanism. Event logs must be accurate, manage complex structures, and be consistent and complete. For the methodology used, not all dimensions of \Cref{fig:ch1_EventQModel} are used to design the logging mechanism.\par This presents the functional requirement of Case Study B's logging points (\ref{fr:loggingPoints}). The multiple logging points introduced inconsistencies in the event logs for the groups of subsystems used. There are studies on improving event log quality, but creating an event log quality model specifically for user-based event logging can increase log quality. \par Some of the defined event log quality model requirements specified in \Cref{fig:ch1_EventQModel} still need to be fully integrated into the method. Some of these requirements can add value to log quality.

\subsection{Maintenance prioritisation}
The maintenance prioritisation (\ref{fr:maintenancePrioritising}) can be improved upon in multiple regards. \Cref{eq:ch2_maintenanceFactorSimplified,eq:ch2_eventNormalised,eq:ch2_priorityNormalised} use the full user-based event logs per subsystem and the total users linked per system as their base variables. \par From the results observed in the case studies in \Cref{sec:ch3_caseStudies}, the base variables had an overwhelming impact on the prioritisation factor. Introducing additional variables that represent alternative factors from the log attributes can improve the accuracy of maintenance prioritisation. An important variable that could have been used is the type of user activity of each case study.

\subsubsection{User activity types}
Throughout the study, the type of user activity was essential to describe what can be classified as a user event. This primary log attribute can add another dimension to the results, as some user activities may be more important than others. \par For software systems, as in Case Study C, where most types of user activity were grouped, the results can be beneficial. There are some differences between some activity types. 

\subsubsection{User and activity parameters}
\par Normalisation is used in \Cref{eq:ch2_maintenanceFactorSimplified,eq:ch2_eventNormalised,eq:ch2_priorityNormalised} to make a comparable scale for both main variables for software maintenance prioritisation. While normalisation yielded similar results, there were numerous outliers on each variable's low and high ends. The data do not follow a specific distribution or pattern as $S_{582}$ of Case Study A with a normalised activity of 1. This subsystem had the highest normalised activity but had the $4^{th}$ highest maintenance priority due to its lower unique active user base. \par Using different techniques to formulate a software maintenance priority factor may have placed seen in $S_{582}$ higher if the effect of total unique users was not used as the primary prioritisation factor for this situation. 

\subsubsection{Use of other classification strategies}
Normalisation was used for maintenance priority calculations. Using other statistical methods can yield improved prioritisation. Only one prioritisation method was used. A need exists to enhance prioritisation by comparing different prioritisation methods.

\section{Conclusion}

\subsection{In summary}
\begin{itemize}
	\item There is a need to improve software maintenance activities in the industry, but software maintenance prioritisation is still a problem for most software developers.
	\item Event-based logging is a proven method for gathering valuable information about a software system.
	\item A log analysis of the utilisation of the software system can provide the evidence required to prioritise software systems based on the extracted log data.
	\item Creating a user-based event logging mechanism to implement a system utilisation log analysis solves the identified problem.
	\item Three different case studies with two further operational use cases verified the methodology designed for this study.
\end{itemize}

\subsection{Conclusion}
Analysing user-based event logs can improve software maintenance resource management by prioritising maintenance tasks through a comprehensive log analysis.