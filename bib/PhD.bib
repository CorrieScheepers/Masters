@article{Gurumdimma2016,
abstract = {{\textcopyright} 2016 IEEE. The use of console logs for error detection in large scale distributed systems has proven to be useful to system administrators. However, such logs are typically redundant and incomplete, making accurate detection very difficult. In an attempt to increase this accuracy, we complement these incomplete console logs with resource usage data, which captures the resource utilisation of every job in the system. We then develop a novel error detection methodology, the CRUDE approach, that makes use of both the resource usage data and console logs. We thus make the following specific technical contributions: we develop (i) a clustering algorithm to group nodes with similar behaviour, (ii) an anomaly detection algorithm to identify jobs with anomalous resource usage, (iii) an algorithm that links jobs with anomalous resource usage with erroneous nodes. We then evaluate our approach using console logs and resource usage data from the Ranger Supercomputer. Our results are positive: (i) our approach detects errors with a true positive rate of about 80{\%}, and (ii) when compared with the well-known Nodeinfo error detection algorithm, our algorithm provides an average improvement of around 85{\%} over Nodeinfo, with a best-case improvement of 250{\%}.},
author = {Gurumdimma, Nentawe and Jhumka, Arshad and Liakata, Maria and Chuah, Edward and Browne, James},
doi = {10.1109/SRDS.2016.017},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/CRUDE Combining Resource Usage Data and Error Logs for Accurate Error Detection in Large-Scale Distributed Systems{\_}Gurumdimma et al.{\_}201.pdf:pdf},
isbn = {9781509035137},
issn = {10609857},
journal = {Proceedings of the IEEE Symposium on Reliable Distributed Systems},
keywords = {anomaly detection,detection,event logs,faults,large-scale HPC systems,resource usage data,unsupervised},
pages = {51--60},
publisher = {IEEE},
title = {{CRUDE: Combining Resource Usage Data and Error Logs for Accurate Error Detection in Large-Scale Distributed Systems}},
year = {2016}
}
@article{Park2017,
abstract = {Today's high-performance computing (HPC) systems are heavily instrumented, generating logs containing information about abnormal events, such as critical conditions, faults, errors and failures, system resource utilization, and about the resource usage of user applications. These logs, once fully analyzed and correlated, can produce detailed information about the system health, root causes of failures, and analyze an application's interactions with the system, providing valuable insights to domain scientists and system administrators. However, processing HPC logs requires a deep understanding of hardware and software components at multiple layers of the system stack. Moreover, most log data is unstructured and voluminous, making it more difficult for system users and administrators to manually inspect the data. With rapid increases in the scale and complexity of HPC systems, log data processing is becoming a big data challenge. This paper introduces a HPC log data analytics framework that is based on a distributed NoSQL database technology, which provides scalability and high availability, and the Apache Spark framework for rapid in-memory processing of the log data. The analytics framework enables the extraction of a range of information about the system so that system administrators and end users alike can obtain necessary insights for their specific needs. We describe our experience with using this framework to glean insights from the log data about system behavior from the Titan supercomputer at the Oak Ridge National Laboratory.},
author = {Park, Byung H. and Hukerikar, Saurabh and Adamson, Ryan and Engelmann, Christian},
doi = {10.1109/CLUSTER.2017.113},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Big data meets HPC log analytics Scalable approach to understanding systems at extreme scale{\_}Park et al.{\_}2017.pdf:pdf},
isbn = {9781538623268},
issn = {15525244},
journal = {Proceedings - IEEE International Conference on Cluster Computing, ICCC},
keywords = {Big data processing,Log data analytics,System monitoring},
pages = {758--765},
title = {{Big data meets HPC log analytics: Scalable approach to understanding systems at extreme scale}},
volume = {2017-Septe},
year = {2017}
}
@article{Thankachan2018,
abstract = {This paper discusses the design of a solution to enable data driven decision making for application support. The design proposes a novel standard for logging within applications. The next phase of the design proposes a novel method to use the standardized logs to map the functioning of an application to a finite state automata. The analytics proposed in the design helps understand issues during application processing, assess the impact of issues and quickly take decisions to resolve the issues. Big data analytics and probabilistic models are used on the historical application logs to further predict issues prior to their occurrence, assess the health of application functioning and be able to proactively act on situations that can lead to errors.},
author = {Thankachan, Karun},
doi = {10.1109/ICICI.2017.8365229},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Data driven decision making for application support{\_}Thankachan{\_}2018.pdf:pdf},
isbn = {9781538640319},
journal = {Proceedings of the International Conference on Inventive Computing and Informatics, ICICI 2017},
keywords = {Big data analytics,Finite state automata,Machine learning,Prescriptive systems,Probabilistic models},
number = {Icici},
pages = {716--720},
title = {{Data driven decision making for application support}},
year = {2018}
}
@article{Slaninova2014,
abstract = {This paper is focused on log files where one log file attribute is an originator of the recorded activity (originator is a person in our case). Hence, based on the similar attributes of people, we are able to construct models which explain certain aspects of a persons behaviour. Moreover, we can extract user profiles based on behaviour and find latent ties between users and between different user groups with similar behaviours. We accomplish this by our new approach using the methods from log mining, business process analysis, complex networks and graph theory. The paper describes the whole process of the approach from the log file to the user graph. The main focus is on the step called 'The finding of user behavioural patterns'.},
author = {Slaninov{\'{a}}, Kateřina},
doi = {10.1109/ISDA.2013.6920751},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/User behavioural patterns and reduced user profiles extracted from log files{\_}Slaninov{\'{a}}{\_}2014.pdf:pdf},
isbn = {9781479935161},
issn = {21647151},
journal = {International Conference on Intelligent Systems Design and Applications, ISDA},
keywords = {analysis of users' behaviour,behavioural patterns,complex networks,user profiles},
pages = {289--294},
publisher = {IEEE},
title = {{User behavioural patterns and reduced user profiles extracted from log files}},
year = {2014}
}
@article{Vaarandi2015,
abstract = {Modern IT systems often produce large volumes of event logs, and event pattern discovery is an important log management task. For this purpose, data mining methods have been suggested in many previous works. In this paper, we present the LogCluster algorithm which implements data clustering and line pattern mining for textual event logs. The paper also describes an open source implementation of LogCluster.},
author = {Vaarandi, Risto and Pihelgas, Mauno},
doi = {10.1109/CNSM.2015.7367331},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/LogCluster - A data clustering and pattern mining algorithm for event logs{\_}Vaarandi, Pihelgas{\_}2015.pdf:pdf},
isbn = {9783901882777},
journal = {Proceedings of the 11th International Conference on Network and Service Management, CNSM 2015},
keywords = {data clustering,data mining,event log analysis,event log clustering,mining patterns from event logs},
pages = {1--7},
title = {{LogCluster - A data clustering and pattern mining algorithm for event logs}},
year = {2015}
}
@article{Cinque2013,
abstract = {Event logs have been widely used over the last three decades to analyze the failure behavior of a variety of systems. Nevertheless, the implementation of the logging mechanism lacks a systematic approach and collected logs are often inaccurate at reporting software failures: This is a threat to the validity of log-based failure analysis. This paper analyzes the limitations of current logging mechanisms and proposes a rule-based approach to make logs effective to analyze software failures. The approach leverages artifacts produced at system design time and puts forth a set of rules to formalize the placement of the logging instructions within the source code. The validity of the approach, with respect to traditional logging mechanisms, is shown by means of around 12,500 software fault injection experiments into real-world systems. {\textcopyright} 2012 IEEE.},
author = {Cinque, Marcello and Cotroneo, Domenico and Pecchia, Antonio},
doi = {10.1109/TSE.2012.67},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Event logs for the analysis of software failures A rule-based approach{\_}Cinque, Cotroneo, Pecchia{\_}2013.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Event log,error detection,logging mechanism,rule-based logging,software failures},
number = {6},
pages = {806--821},
publisher = {IEEE},
title = {{Event logs for the analysis of software failures: A rule-based approach}},
volume = {39},
year = {2013}
}
@article{Pecchia2015,
abstract = {{\textcopyright} 2015 IEEE. Practitioners widely recognize the importance of event logging for a variety of tasks, such as accounting, system measurements and troubleshooting. Nevertheless, in spite of the importance of the tasks based on the logs collected under real workload conditions, event logging lacks systematic design and implementation practices. The implementation of the logging mechanism strongly relies on the human expertise. This paper proposes a measurement study of event logging practices in a critical industrial domain. We assess a software development process at Selex ES, a leading Finmeccanica company in electronic and information solutions for critical systems. Our study combines source code analysis, inspection of around 2.3 millions log entries, and direct feedback from the development team to gain process-wide insights ranging from programming practices, logging objectives and issues impacting log analysis. The findings of our study were extremely valuable to prioritize event logging reengineering tasks at Selex ES.},
author = {Pecchia, Antonio and Cinque, Marcello and Carrozza, Gabriella and Cotroneo, Domenico},
doi = {10.1109/ICSE.2015.145},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Industry Practices and Event Logging Assessment of a Critical Software Development Process{\_}Pecchia et al.{\_}2015.pdf:pdf},
isbn = {9781479919345},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Coding practices,Development process,Event logging,Industry domain,Source code analysis},
pages = {169--178},
publisher = {IEEE},
title = {{Industry Practices and Event Logging: Assessment of a Critical Software Development Process}},
volume = {2},
year = {2015}
}
@article{Song2008,
abstract = {Although information systems support a wide range of recreational and social activities, the pattern of users' behavior in such systems is not clear. We extend the process mining technology to work on common event logs that have no workflow cases reference, and name the new technology as behavior pattern mining. This paper gives out a brief survey on issues, challenges, approaches and related tools in behavior pattern mining area, and compares behavior pattern mining with workflow mining technology, which is the other sub field of process mining. To reply the problems proposed by behavior pattern mining, the process mining area gets its new motivation.},
author = {Song, Jinliang and Luo, Tiejian and Chen, Su},
doi = {10.1109/ICNSC.2008.4525516},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Behavior pattern mining Apply process mining technology to common event logs of information systems{\_}Song, Luo, Chen{\_}2008.pdf:pdf},
isbn = {9781424416851},
journal = {Proceedings of 2008 IEEE International Conference on Networking, Sensing and Control, ICNSC},
pages = {1800--1805},
title = {{Behavior pattern mining: Apply process mining technology to common event logs of information systems}},
year = {2008}
}
@article{Razavi2008,
author = {Razavi, Ali and Kontogiannis, Kostas},
doi = {10.1109/COMPSAC.2008.81},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Pattern and policy driven log analysis for software monitoring{\_}Razavi, Kontogiannis{\_}2008.pdf:pdf},
isbn = {9780769532622},
issn = {07303157},
journal = {Proceedings - International Computer Software and Applications Conference},
pages = {108--111},
title = {{Pattern and policy driven log analysis for software monitoring}},
year = {2008}
}
@article{Song2017,
abstract = {The aligning of event logs with process models is of great significance for process mining to enable conformance checking, process enhancement, performance analysis, and trace repairing. Since process models are increasingly complex and event logs may deviate from process models by exhibiting redundant, missing, and dislocated events, it is challenging to determine the optimal alignment for each event sequence in the log, as this problem is NP-hard. Existing approaches utilize the cost-based A∗ algorithm to address this problem. However, scalability is often not considered, which is especially important when dealing with industrial-sized problems. In this paper, by taking advantage of the structural and behavioral features of process models, we present an efficient approach which leverages effective heuristics and trace replaying to significantly reduce the overall search space for seeking the optimal alignment. We employ real-world business processes and their traces to evaluate the proposed approach. Experimental results demonstrate that our approach works well in most cases, and that it outperforms the state-of-the-art approach by up to 5 orders of magnitude in runtime efficiency. {\textcopyright} 2016 IEEE.},
author = {Song, Wei and Xia, Xiaoxu and Jacobsen, Hans Arno and Zhang, Pengcheng and Hu, Hao},
doi = {10.1109/TSC.2016.2601094},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Efficient Alignment Between Event Logs and Process Models{\_}Song et al.{\_}2017.pdf:pdf},
issn = {19391374},
journal = {IEEE Transactions on Services Computing},
keywords = {Event logs,alignment,process decomposition,process models,trace replaying,trace segmentation},
number = {1},
pages = {136--149},
publisher = {IEEE},
title = {{Efficient Alignment Between Event Logs and Process Models}},
volume = {10},
year = {2017}
}
@inproceedings{Ferreira2009,
abstract = {Existing process mining techniques are able to discover process models from event logs where each event is known to have been produced by a given process instance. In this paper we remove this restriction and address the problem of discovering the process model when the event log is provided as an unlabelled stream of events. Using a probabilistic approach, it is possible to estimate the model by means of an iterative Expectaction-Maximization procedure. The same procedure can be used to find the case id. in unlabelled event logs. A series of experiments show how the proposed technique performs under varying conditions and in the presence of certain workflow patterns. Results are presented for a running example based on a technical support process.},
author = {Ferreira, Diogo R. and Gillblad, Daniel},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-03848-8_11},
isbn = {3642038476},
issn = {03029743},
pages = {143--158},
title = {{Discovering process models from unlabelled event logs}},
volume = {5701 LNCS},
year = {2009}
}
@inproceedings{Chen2019,
author = {Chen, Boyuan},
doi = {10.1109/icse-companion.2019.00080},
pages = {194--197},
title = {{Improving the Software Logging Practices in DevOps}},
year = {2019}
}
@article{Krol2008,
author = {Krol, D and Scigajlo, M and Trawi{\'{n}}ski, Bogda},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/I Vestigatio of I Ter Et System User Behaviour Usi G Cluster a Alysis{\_}Krol, Scigajlo, Trawi{\'{n}}ski{\_}2008.pdf:pdf},
isbn = {9781424420964},
journal = {Machine Learning},
keywords = {clustering,hcm,server log,user activity,web system},
number = {July},
pages = {12--15},
title = {{I Vestigatio of I Ter Et System User Behaviour Usi G Cluster a Alysis}},
year = {2008}
}
@article{Rong2018,
abstract = {Background: Logs are the footprints that software systems produce during runtime, which can be used to understand the dynamic behavior of these software systems. To generate logs, logging practice is accepted by developers to place logging statements in the source code of software systems. Compared to the great number of studies on log analysis, the research on logging practice is relatively scarce, which raises a very critical question, i.e. as the original intention, can current logging practice support capturing the behavior of software systems effectively? Aims: To answer this question, we first need to understand how logging practices are implemented these software projects. Method: In this paper, we carried out an empirical study to explore the logging practice in open source software projects so as to establish a basic understanding on how logging practice is applied in real world software projects. The density, log level (what to log?) and context (where to log?) are measured for our study. Results: Based on the evidence we collected in 28 top open source projects, we find the logging practice is adopted highly inconsistently among different developers both across projects and even within one project in terms of the density and log levels of logging statements. However, the choice of what context the logging statements to place is consistent to a fair degree. Conclusion: Both the inconsistency in density and log level and the convergence of context have forced us to question whether it is a reliable means to understand the runtime behavior of software systems via analyzing the logs produced by the current logging practice.},
author = {Rong, Guoping and Gu, Shenghui and Zhang, He and Shao, Dong and Liu, Wanggen},
doi = {10.1109/ASWEC.2018.00031},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/How is logging practice implemented in open source software projects A preliminary exploration{\_}Rong et al.{\_}2018.pdf:pdf},
isbn = {9781728112411},
journal = {Proceedings - 25th Australasian Software Engineering Conference, ASWEC 2018},
keywords = {Empirical study,Java-based,Log,Logging practice},
pages = {171--180},
publisher = {IEEE},
title = {{How is logging practice implemented in open source software projects? A preliminary exploration}},
year = {2018}
}
@article{Rong2018a,
abstract = {Background: Logging practice is a critical activity in software development, which aims to offer significant information to understand the runtime behavior of software systems and support better software maintenance. There have been many relevant studies dedicated to logging practice in software en- gineering recently, yet it lacks a systematic understanding to the adoption state of logging practice in industry and research progress in academia. Objective: This study aims to synthesize relevant studies on the logging practice and portray a big picture of logging practice in software engineering so as to understand current adoption status and identify research opportunities. Method: We carried out a systematic review on the relevant studies on logging practice in software engineering. Results: Our study identified 41 primary studies relevant to logging practice. Typical findings are: (1) Logging practice attracts broad interests among researchers in many concrete research areas. (2) Logging practice occurred in many development types, among which the development of fault tolerance systems is the most adopted type. (3) Many challenges exist in current logging practice in software engineering, e.g., tradeoff between logging overhead and analysis cost, where and what to log, balance between enough logging and system performance, etc. Conclusion: Results show that logging practice plays a vital role in various applications for diverse purposes. However, there are many challenges and problems to be solved. Therefore, various novel techniques are necessary to guide developers conducting logging practice and improve the performance and efficiency of logging practice.},
author = {Rong, Guoping and Zhang, Qiuping and Liu, Xinbei and Gu, Shenghiu},
doi = {10.1109/APSEC.2017.61},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/A Systematic Review of Logging Practice in Software Engineering{\_}Rong et al.{\_}2018.pdf:pdf},
isbn = {9781538636817},
issn = {15301362},
journal = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
keywords = {Logging Practice,Software Engineering,Systematic Literature Review},
pages = {534--539},
title = {{A Systematic Review of Logging Practice in Software Engineering}},
volume = {2017-Decem},
year = {2018}
}
@article{Li2018,
abstract = {Logging statements are used to record valuable runtime information about applications. Each logging statement is assigned a log level such that users can disable some verbose log messages while allowing the printing of other important ones. However, prior research finds that developers often have difficulties when determining the appropriate level for their logging statements. In this paper, we propose an approach to help developers determine the appropriate log level when they add a new logging statement. We analyze the development history of four open source projects (Hadoop, Directory Server, Hama, and Qpid), and leverage ordinal regression models to automatically suggest the most appropriate level for each newly-added logging statement. First, we find that our ordinal regression model can accurately suggest the levels of logging statements with an AUC (area under the curve; the higher the better) of 0.75 to 0.81 and a Brier score (the lower the better) of 0.44 to 0.66, which is better than randomly guessing the appropriate log level (with an AUC of 0.50 and a Brier score of 0.80 to 0.83) or naively guessing the log level based on the proportional distribution of each log level (with an AUC of 0.50 and a Brier score of 0.65 to 0.76). Second, we find that the characteristics of the containing block of a newly-added logging statement, the existing logging statements in the containing source code file, and the content of the newly-added logging statement play important roles in determining the appropriate log level for that logging statement.},
author = {Li, Heng and Shang, Weiyi and Hassan, Ahmed E.},
doi = {10.1109/SANER.2018.8330234},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Which log level should developers choose for a new logging statement (journal-first abstract){\_}Li, Shang, Hassan{\_}2018.pdf:pdf},
isbn = {9781538649695},
journal = {25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},
keywords = {log level,ordinal regression model,software logging},
number = {4},
pages = {468},
publisher = {IEEE},
title = {{Which log level should developers choose for a new logging statement? (journal-first abstract)}},
volume = {2018-March},
year = {2018}
}
@article{Pathan2014,
abstract = {Data mining is the process of finding correlations in the relational databases. There are different techniques for identifying malicious database transactions. Many existing approaches which profile is SQL query structures and database user activities to detect intrusion, the log mining approach is the automatic discovery for identifying anomalous database transactions. Mining of the Data is very helpful to end users for extracting useful business information from large database. Multi-level and multi-dimensional data mining are employed to discover data item dependency rules, data sequence rules, domain dependency rules, and domain sequence rules from the database log containing legitimate transactions. Database transactions that do not comply with the rules are identified as malicious transactions. The log mining approach can achieve desired true and false positive rates when the confidence and support are set up appropriately. The implemented system incrementally maintain the data dependency rule sets and optimize the performance of the intrusion detection process. {\textcopyright} 2014 IEEE.},
author = {Pathan, Apashabi Chandkhan and Potey, Madhuri A.},
doi = {10.1109/ICESC.2014.50},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Detection of malicious transaction in database using log mining approach{\_}Pathan, Potey{\_}2014.pdf:pdf},
isbn = {9781479921027},
journal = {Proceedings - International Conference on Electronic Systems, Signal Processing, and Computing Technologies, ICESC 2014},
keywords = {Data Mining,Database security,Intrusion Detection},
pages = {262--265},
publisher = {IEEE},
title = {{Detection of malicious transaction in database using log mining approach}},
year = {2014}
}
@article{Cui2003,
abstract = {Queries to search engines on the Web are usually short. They do not provide sufficient information for an effective selection of relevant documents. Previous research has proposed the utilization of query expansion to deal with this problem. However, expansion terms are usually determined on term co-occurrences within documents. In this study, we propose a new method for query expansion based on user interactions recorded in user logs. The central idea is to extract correlations between query terms and document terms by analyzing user logs. These correlations are then used to select high-quality expansion terms for new queries. Compared to previous query expansion methods, ours takes advantage of the user judgments implied in user logs. The experimental results show that the log-based query expansion method can produce much better results than both the classical search method and the other query expansion methods.},
author = {Cui, Hang and Wen, Ji Rong and Nie, Jian Yun and Ma, Wei Ying},
doi = {10.1109/TKDE.2003.1209002},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Query expansion by mining user logs{\_}Cui et al.{\_}2003.pdf:pdf},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Information retrieval,Probabilistic model,Query expansion,Search engine,User log},
number = {4},
pages = {829--839},
title = {{Query expansion by mining user logs}},
volume = {15},
year = {2003}
}
@article{Potey2013,
abstract = {Query log is the pouch of valuable information that records user's search queries and related actions on the internet. By mining the recorded information, it is possible to exploit the user's underlying goals, preferences, interests, search behaviors and implicit feedback. The wealth of mined information can be used in many applications such as query log analysis, query recommendation, query reformulation, query intent identification and many more to improve performance of search engine by providing more relevant results. Over the past decade, there has been tremendous work done for improving search engine results to flourish the users for searching. This paper reviews and compares some of the available methods to give an insight into the area of query log processing for information retrieval. Our approach classifies web query intent based on knowledge extraction from query log analysis. {\textcopyright} 2013 IEEE.},
author = {Potey, Madhuri A. and Patel, Dhanashri A. and Sinha, P. K.},
doi = {10.1109/IAdCC.2013.6514421},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/A survey of query log processing techniques and evaluation of web query intent identification{\_}Potey, Patel, Sinha{\_}2013.pdf:pdf},
isbn = {9781467345286},
journal = {Proceedings of the 2013 3rd IEEE International Advance Computing Conference, IACC 2013},
keywords = {Query Chains,Query Intent Identification,Query Log Processing,Query Recommendation,Query Reformulation},
pages = {1330--1335},
publisher = {IEEE},
title = {{A survey of query log processing techniques and evaluation of web query intent identification}},
year = {2013}
}
@article{Zhu2019,
abstract = {Logs are imperative in the development and maintenance process of many software systems. They record detailed runtime information that allows developers and support engineers to monitor their systems and dissect anomalous behaviors and errors. The increasing scale and complexity of modern software systems, however, make the volume of logs explodes. In many cases, the traditional way of manual log inspection becomes impractical. Many recent studies, as well as industrial tools, resort to powerful text search and machine learning-based analytics solutions. Due to the unstructured nature of logs, a first crucial step is to parse log messages into structured data for subsequent analysis. In recent years, automated log parsing has been widely studied in both academia and industry, producing a series of log parsers by different techniques. To better understand the characteristics of these log parsers, in this paper, we present a comprehensive evaluation study on automated log parsing and further release the tools and benchmarks for easy reuse. More specifically, we evaluate 13 log parsers on a total of 16 log datasets spanning distributed systems, supercomputers, operating systems, mobile systems, server applications, and standalone software. We report the benchmarking results in terms of accuracy, robustness, and efficiency, which are of practical importance when deploying automated log parsing in production. We also share the success stories and lessons learned in an industrial application at Huawei. We believe that our work could serve as the basis and provide valuable guidance to future research and deployment of automated log parsing.},
author = {Zhu, Jieming and He, Shilin and Liu, Jinyang and He, Pinjia and Xie, Qi and Zheng, Zibin and Lyu, Michael R.},
doi = {10.1109/icse-seip.2019.00021},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Tools and Benchmarks for Automated Log Parsing{\_}Zhu et al.{\_}2019.pdf:pdf},
isbn = {9781728117607},
journal = {2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
pages = {121--130},
publisher = {IEEE},
title = {{Tools and Benchmarks for Automated Log Parsing}},
year = {2019}
}
@article{Dhanalakshmi2016,
abstract = {The increased on-line applications are leading to exponential growth of the web content. Most of the business organizations are interested to know the web user behavior to enhance their business. In this context, users navigation in static and dynamic web applications plays an important role in understanding user's interests. The static mining techniques may not be suitable as it is for dynamic web log files and decision making. Traditional web log preprocessing approaches and weblog usage patterns have limitations to analyze the content relationship with the browsing history This paper, focuses on various static web log preprocessing and mining techniques and their applicable limitations for dynamic web mining.},
author = {Dhanalakshmi, P. and Ramani, K. and Reddy, B. Eswara},
doi = {10.1109/IACC.2016.35},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/The Research of Preprocessing and Pattern Discovery Techniques on Web Log Files{\_}Dhanalakshmi, Ramani, Reddy{\_}2016.pdf:pdf},
isbn = {9781467382861},
journal = {Proceedings - 6th International Advanced Computing Conference, IACC 2016},
keywords = {Asscociation rules,Graph models,Navigation patterns,Static Logs,Web log},
pages = {139--145},
publisher = {IEEE},
title = {{The Research of Preprocessing and Pattern Discovery Techniques on Web Log Files}},
year = {2016}
}
@article{Lei2018,
author = {Lei, Xuezhi},
doi = {10.1109/ESAIC.2018.00052},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Modeling and intelligent analysis of web user behavior of web user behavior{\_}Lei{\_}2018.pdf:pdf},
isbn = {9781538680285},
journal = {Proceedings - 2018 International Conference on Engineering Simulation and Intelligent Control, ESAIC 2018},
keywords = {Data mining,Intelligent analysis,Interest degree,User behavior},
pages = {192--195},
title = {{Modeling and intelligent analysis of web user behavior of web user behavior}},
year = {2018}
}
@article{Kumar2017,
author = {Kumar, Manoj},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Web Log Expert Tool{\_}Kumar{\_}2017.pdf:pdf},
isbn = {9781509056866},
keywords = {web server log,web usage mining},
pages = {296--301},
title = {{Web Log Expert Tool}},
year = {2017}
}
@article{Lu2019,
abstract = {A decision problem, according to traditional principles, is approached by finding an optimal solution to an analytical programming decision model, which is known as model-driven decision-making. The fidelity of the model determines the quality and reliability of the decision-making; however, the intrinsic complexity of many real-world decision problems leads to significant model mismatch or infeasibility in deriving a model using the first principle. To overcome the challenges that are present in the big data era, both researchers and practitioners emphasize the importance of making decisions that are backed up by data related to decision tasks, a process called data-driven decision-making (D3M). By building on data science, not only can decision models be predicted in the presence of uncertainty or unknown dynamics, but also inherent rules or knowledge can be extracted from data and directly utilized to generate decision solutions. This position paper systematically discusses the basic concepts and prevailing techniques in data-driven decision-making and clusters-related developments in technique into two main categories: programmable data-driven decision-making (P-D3M) and nonprogrammable data-driven decision-making (NP-D3M). This paper establishes a D3M technical framework, main methodologies, and approaches for both categories of D3M, as well as identifies potential methods and procedures for using data to support decision-making. It also provides examples of how D3M is implemented in practice and identifies five further research directions in the D3M area. We believe that this paper will directly support researchers and professionals in their understanding of the fundamentals of D3M and of the developments in technical methods},
author = {Lu, Jie and Yan, Zheng and Han, Jialin and Zhang, Guangquan},
doi = {10.1109/tetci.2019.2915813},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Data-Driven Decision-Making (D 3 M) Framework, Methodology, and Directions{\_}Lu et al.{\_}2019.pdf:pdf},
journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
number = {4},
pages = {286--296},
publisher = {IEEE},
title = {{ Data-Driven Decision-Making (D 3 M): Framework, Methodology, and Directions }},
volume = {3},
year = {2019}
}
@article{Lenarduzzi2017,
abstract = {Software maintenance has dramatically evolved in the last four decades, to cope with the continuously changing software development models, and programming languages and adopting increasingly advanced prediction models. In this work, we present the initial results of a Systematic Literature Review (SLR), highlighting the evolution of the metrics and models adopted in the last forty years.},
author = {Lenarduzzi, Valentina and Sillitti, Alberto and Taibi, Davide},
doi = {10.1109/ICSE-C.2017.122},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Analyzing Forty years of software maintenance models{\_}Lenarduzzi, Sillitti, Taibi{\_}2017.pdf:pdf},
isbn = {9781538615898},
journal = {Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017},
keywords = {Software Maintenance,Systematic Literature Review},
pages = {146--148},
publisher = {IEEE},
title = {{Analyzing Forty years of software maintenance models}},
year = {2017}
}
@article{Sneed2004,
abstract = {The purpose of this essay is to present a costing model for software maintenance and evolution based on a separation of fixed and variable costs. There has always been a problem in distinguishing between the maintenance activities covered by the standard maintenance fee and those charged extra to the user. Separating these two types of costs is essential to every maintenance operation to prevent costs from getting out of control. In this paper the author proposes a solution, which can lead to better cost estimations and a financially more stable maintenance operation. Particular emphasis is placed on a sharp division between work done to maintain the system functionality as it is and work done to enhance that functionality. {\textcopyright} 2004 IEEE.},
author = {Sneed, Harry M.},
doi = {10.1109/ICSM.2004.1357810},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/A cost model for software maintenance {\&} evolution{\_}Sneed{\_}2004.pdf:pdf},
journal = {IEEE International Conference on Software Maintenance, ICSM},
keywords = {Maintenance Cost Estimation,Software Life Cycle Costing Models,Software Maintenance and Evolution,Software Product Management},
pages = {264--273},
publisher = {IEEE},
title = {{A cost model for software maintenance {\&} evolution}},
year = {2004}
}
@inproceedings{Atterer2006,
abstract = {In this paper, we investigate how detailed tracking of user interaction can be monitored using standard web technologies. Our motivation is to enable implicit interaction and to ease usability evaluation of web applications outside the lab. To obtain meaningful statements on how users interact with a web application, the collected information needs to be more detailed and fine-grained than that provided by classical log files. We focus on tasks such as classifying the user with regard to computer usage proficiency or making a detailed assessment of how long it took users to fill in fields of a form. Additionally, it is important in the context of our work that usage tracking should not alter the user's experience and that it should work with existing server and browser setups. We present an implementation for detailed tracking of user actions on web pages. An HTTP proxy modifies HTML pages by adding JavaScript code before delivering them to the client. This JavaScript tracking code collects data about mouse movements, keyboard input and more. We demonstrate the usefulness of our approach in a case study.},
author = {Atterer, Richard and Wnuk, Monika and Schmidt, Albrecht},
booktitle = {Proceedings of the 15th International Conference on World Wide Web},
doi = {10.1145/1135777.1135811},
isbn = {1595933239},
keywords = {HTTP proxy,Implicit interaction,Mouse tracking,User activity tracking,Website usability evaluation},
title = {{Knowing the user's every move: User activity tracking for website usability evaluation and implicit interaction}},
year = {2006}
}
@inproceedings{Mueller2001,
abstract = {Conventional web interfaces respond to and consider only mouse clicks when defining a user model. We have extended this and take into account all mouse movements on a page as an additional layer of information for inferring user interest. We have developed a straightforward way to record all mouse movements on a page, and conducted a user study to analyze and investigate mouse behavior trends. We found certain mouse behaviors, common across many users, which are useful for content providers in increasing the effectiveness of their interface design. Copyright {\textcopyright} 2012 ACM, Inc.},
author = {Mueller, Florian and Lockerd, Andrea},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/634067.634233},
isbn = {1581133405},
keywords = {Context,Mouse activity,Mouse movements,User study,Web mouse tracking design guidelines,Web page design},
title = {{Cheese: Tracking mouse movement activity on websites, a tool for user modeling}},
year = {2001}
}
@article{Dalpiaz2018,
abstract = {90{\%} of agile practitioners employ user stories for capturing requirements. Of these, 70{\%} follow a simple template when creating user stories: As a {\textless}role{\textgreater} I want to {\textless}action{\textgreater}, [so that {\textless}benefit{\textgreater}]. User stories' popularity among practitioners and their simple yet strict structure make them ideal candidates for automatic reasoning based on natural language processing. In our research, we have found that circa 50{\%} of real-world user stories contain easily preventable errors that may endanger their potential. To alleviate this problem, we have created methods, theories and tools that support creating better user stories. This tutorial combines our previous work into a pipeline for working with user stories: (1) The basics of creating user stories, and their use in requirements engineering; (2) How to improve user story quality with the Quality User Story Framework and AQUSA tool; and (3) How to generate conceptual models from user stories using the Visual Narrator and the Interactive Narrator tools. Our toolset is demonstrated with results obtained from 20+ software companies employing user stories.},
author = {Dalpiaz, Fabiano and Brinkkemper, Sjaak},
doi = {10.1109/RE.2018.00075},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Agile requirements engineering with user stories{\_}Dalpiaz, Brinkkemper{\_}2018.pdf:pdf},
isbn = {9781538674185},
journal = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
keywords = {Agile requirements engineering,Natural language processing,User stories},
pages = {506--507},
title = {{Agile requirements engineering with user stories}},
year = {2018}
}
@article{Niu2018,
abstract = {This article summarizes the RE in the Age of Continuous Deployment panel at the 25th IEEE International Requirements Engineering Conference. It highlights two synergistic points (user stories and linguistic tooling) and one challenge (nonfunctional requirements) in fast-paced, agile-like projects, and recommends how to carry on the dialogue.},
author = {Niu, Nan and Brinkkemper, Sjaak and Franch, Xavier and Partanen, Jari and Savolainen, Juha},
doi = {10.1109/MS.2018.1661332},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Requirements engineering and continuous deployment{\_}Niu et al.{\_}2018.pdf:pdf},
issn = {07407459},
journal = {IEEE Software},
keywords = {25th IEEE International Requirements Engineering C,RE in the Age of Continuous Deployment,agile software development,continuous deployment,linguistic tooling,nonfunctional requirements,requirements engineering,software development,software engineering,software requirements,user stories},
number = {2},
pages = {86--90},
publisher = {IEEE},
title = {{Requirements engineering and continuous deployment}},
volume = {35},
year = {2018}
}
@article{Tian2017,
abstract = {A web service reliability test method for C/S architecture software based on log analysis is presented in this paper. In this method, the software usage model is constructed automatically to describe the real situation on the users' access to the web service by Markov chain. The test cases are generated according to Random Walk and applied to software reliability test. In the experiment process, MTBF (focusing on server crash) was chosen to be the software reliability evaluation index. Through the testing and analysis of a real web software, MTBF obtained by testing result is similar to that from the realistic log, and the web service reliability test method is validated.},
author = {Tian, Xuetao and Li, Honghui and Liu, Feng},
doi = {10.1109/QRS-C.2017.38},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Web Service Reliability Test Method Based on Log Analysis{\_}Tian, Li, Liu{\_}2017.pdf:pdf},
isbn = {9781538620724},
journal = {Proceedings - 2017 IEEE International Conference on Software Quality, Reliability and Security Companion, QRS-C 2017},
keywords = {Log Analysis,Markov Usage Model,Reliability Test,Test Cases,Web Service},
pages = {195--199},
publisher = {IEEE},
title = {{Web Service Reliability Test Method Based on Log Analysis}},
year = {2017}
}
@article{Waqar2017,
abstract = {Tracking users' posting activities in online classified ads and understanding the dynamics of their behavior is a topic of great importance with many implications. However, some of the underlying problems associated with modeling users and detecting their behavioral changes due to temporal and spatial variations have not been well-studied. In this paper, we develop a probabilistic model of user behavior based on the ads the user posts and the categories in which the ads are posted. The model can track some of the temporal changes in behavior, as revealed by our experiments on two classes of users monitored over a period of almost a year. We study the association between post categories and user groups, and show how temporal and seasonal changes can be detected. We further investigate a generative model for ad posts, based on user locations, and provide some evidence showing that the model is promising and that some interesting relationships can be identified.},
author = {Waqar, Muhammad and Rafiei, Davood},
doi = {10.1109/WI.2016.0088},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Tracking User Activities and Marketplace Dynamics in Classified Ads{\_}Waqar, Rafiei{\_}2017.pdf:pdf},
isbn = {9781509044702},
journal = {Proceedings - 2016 IEEE/WIC/ACM International Conference on Web Intelligence, WI 2016},
keywords = {Classified ads,Temporal analysis,User modeling,User tracking},
pages = {522--525},
publisher = {IEEE},
title = {{Tracking User Activities and Marketplace Dynamics in Classified Ads}},
year = {2017}
}
@article{Port2017,
abstract = {NASA has been successfully sustaining the continuous operation of its critical navigation software systems for over 12 years. To accomplish this, NASA scientists must continuously monitor their process, report on current system quality, forecast maintenance effort, and sustain required staffing levels. This report presents some examples of the use of a robust software metrics and analytics program that enables actionable strategic maintenance management of a critical system (Monte) in a timely, economical, and risk-controlled fashion. This article is part of a special issue on Actionable Analytics for Software Engineering.},
author = {Port, Dan and Taber, Bill},
doi = {10.1109/MS.2017.4541055},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Actionable Analytics for Strategic Maintenance of Critical Software An Industry Experience Report{\_}Port, Taber{\_}2017.pdf:pdf},
issn = {07407459},
journal = {IEEE Software},
keywords = {Monte,NASA,critical systems,navigation systems,reliability,software analytics,software development,software engineering,software maintenance},
number = {1},
pages = {58--63},
publisher = {IEEE},
title = {{Actionable Analytics for Strategic Maintenance of Critical Software: An Industry Experience Report}},
volume = {35},
year = {2017}
}
@article{Shahid2016,
abstract = {Change impact is an important issue in software maintenance phase. As retesting is required over a software change, there is a need to keep track of software impact associated with changes. Even a small software change can ripple through to cause a large unintended impact elsewhere in the system that makes it difficult to identify the affected functionalities. The impact after changes demands for a special traceability approach. This paper presents a new approach and prototype tool, Hybrid Coverage Analysis Tool (HYCAT), as a proof of concept to support the software manager or maintainers to manage impact analysis and its related traceability before and after a change in any software artifact. The proposed approach was then evaluated using a case study, On-Board Automobile (OBA), and experimentation. The traceability output before and after changes were produced and analyzed to capture impact analysis. The results of the evaluation show that the proposed approach has achieved some promising output and remarkable understanding as compared to existing approaches.},
author = {Shahid, Muhammad and Ibrahim, Suhaimi},
doi = {10.1109/IBCAST.2016.7429908},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Change impact analysis with a software traceability approach to support software maintenance{\_}Shahid, Ibrahim{\_}2016.pdf:pdf},
isbn = {9781467391276},
journal = {Proceedings of 2016 13th International Bhurban Conference on Applied Sciences and Technology, IBCAST 2016},
keywords = {impact analysis,software change,software maintenance,software traceability},
pages = {391--396},
title = {{Change impact analysis with a software traceability approach to support software maintenance}},
year = {2016}
}
@article{Galster2019,
author = {Galster, Matthias and Treude, Christoph and Blincoe, Kelly},
doi = {10.1109/ICSME.2019.00060},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Supporting Software Architecture Maintenance by Providing Task-specific Recommendations{\_}Galster, Treude, Blincoe{\_}2019.pdf:pdf},
journal = {2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
pages = {370--372},
publisher = {IEEE},
title = {{Supporting Software Architecture Maintenance by Providing Task-specific Recommendations}},
year = {2019}
}
@article{Levin2019,
abstract = {Lehman's Laws teach us that a software system will become progressively less satisfying to its users over time, unless it is continually adapted to meet new needs. A line of previous works sought to better understand software maintenance by studying how commits can be classified into three main software maintenance activities. Corrective: fault fixing; Perfective: system improvements; Adaptive: new feature introduction. In this work we suggest visualizations for exploring software maintenance activities in both project and individual developer scopes. We demonstrate our approach using a prototype we have built using the Shiny R framework. In addition, we have also published our prototype as an online demo. This demo allows users to explore the maintenance activities of a number of popular open source projects. We believe that the visualizations we provide can assist practitioners in monitoring and maintaining the health of software projects. In particular, they can be useful for identifying general imbalances, peaks, deeps and other anomalies in projects' and developers' maintenance activities.},
author = {Levin, Stanislav and Yehudai, Amiram},
doi = {10.1109/VISSOFT.2019.00021},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Visually exploring software maintenance activities{\_}Levin, Yehudai{\_}2019.pdf:pdf},
isbn = {9781728149394},
journal = {Proceedings - 7th IEEE Working Conference on Software Visualization, VISSOFT 2019},
keywords = {Predictive Modeling,Software Evolution,Software Maintenance},
pages = {110--114},
publisher = {IEEE},
title = {{Visually exploring software maintenance activities}},
year = {2019}
}
