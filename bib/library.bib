Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Zhu2015,
abstract = {Logging is a common programming practice of practical importance to collect system runtime information for postmortem analysis. Strategic logging placement is desired to cover necessary runtime information without incurring unintended consequences (e.g., performance overhead, trivial logs). However, in current practice, there is a lack of rigorous specifications for developers to govern their logging behaviours. Logging has become an important yet tough decision which mostly depends on the domain knowledge of developers. To reduce the effort on making logging decisions, in this paper, we propose a "learning to log" framework, which aims to provide informative guidance on logging during development. As a proof of concept, we provide the design and implementation of a logging suggestion tool, LogAdvisor, which automatically learns the common logging practices on where to log from existing logging instances and further leverages them for actionable suggestions to developers. Specifically, we identify the important factors for determining where to log and extract them as structural features, textual features, and syntactic features. Then, by applying machine learning techniques (e.g., feature selection and classifier learning) and noise handling techniques, we achieve high accuracy of logging suggestions. We evaluate LogAdvisor on two industrial software systems from Microsoft and two open-source software systems from GitHub (totally 19.1M LOC and 100.6K logging statements). The encouraging experimental results, as well as a user study, demonstrate the feasibility and effectiveness of our logging suggestion tool. We believe our work can serve as an important first step towards the goal of "learning to log".},
author = {Zhu, Jieming and He, Pinjia and Fu, Qiang and Zhang, Hongyu and Lyu, Michael R. and Zhang, Dongmei},
doi = {10.1109/ICSE.2015.60},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Learning to log Helping developers make informed logging decisions{\_}Zhu et al.{\_}2015.pdf:pdf},
isbn = {9781479919345},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
pages = {415--425},
publisher = {IEEE},
title = {{Learning to log: Helping developers make informed logging decisions}},
volume = {1},
year = {2015}
}
@article{Antolic2008,
abstract = {This paper gives an overview of possible Key Performance Indicators (KPI) that can be used for software process efficiency evaluation. The overview is based on currently used KPIs in software development projects on CPP platform. The most important KPIs are analyzed, and their usage in the process efficiency evaluation is discussed. The outcome of the measurement is used to initiate further process adjustments and improvements. In addition, there is possibility to perform benchmarking between different development projects, and based on collected data easier search for best practices in the projects that can be broadly implemented. Some proposals and future directions in the area of process measurement are given. {\textcopyright} 2008 by Mipro.},
author = {Antoli{\'{c}}, {\v{Z}}},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/An example of using key performance indicators for software development process efficiency evaluation{\_}Antoli{\'{c}}{\_}2008.pdf:pdf},
isbn = {9789532330373},
journal = {MIPRO 2008 - 31st International Convention Proceedings: Telecommunications and Information},
pages = {156--161},
title = {{An example of using key performance indicators for software development process efficiency evaluation}},
volume = {2},
year = {2008}
}
@article{Rong2018a,
abstract = {Background: Logging practice is a critical activity in software development, which aims to offer significant information to understand the runtime behavior of software systems and support better software maintenance. There have been many relevant studies dedicated to logging practice in software en- gineering recently, yet it lacks a systematic understanding to the adoption state of logging practice in industry and research progress in academia. Objective: This study aims to synthesize relevant studies on the logging practice and portray a big picture of logging practice in software engineering so as to understand current adoption status and identify research opportunities. Method: We carried out a systematic review on the relevant studies on logging practice in software engineering. Results: Our study identified 41 primary studies relevant to logging practice. Typical findings are: (1) Logging practice attracts broad interests among researchers in many concrete research areas. (2) Logging practice occurred in many development types, among which the development of fault tolerance systems is the most adopted type. (3) Many challenges exist in current logging practice in software engineering, e.g., tradeoff between logging overhead and analysis cost, where and what to log, balance between enough logging and system performance, etc. Conclusion: Results show that logging practice plays a vital role in various applications for diverse purposes. However, there are many challenges and problems to be solved. Therefore, various novel techniques are necessary to guide developers conducting logging practice and improve the performance and efficiency of logging practice.},
author = {Rong, Guoping and Zhang, Qiuping and Liu, Xinbei and Gu, Shenghiu},
doi = {10.1109/APSEC.2017.61},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/A Systematic Review of Logging Practice in Software Engineering{\_}Rong et al.{\_}2018.pdf:pdf},
isbn = {9781538636817},
issn = {15301362},
journal = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
keywords = {Logging Practice,Software Engineering,Systematic Literature Review},
pages = {534--539},
title = {{A Systematic Review of Logging Practice in Software Engineering}},
volume = {2017-Decem},
year = {2018}
}
@article{Waqar2017,
abstract = {Tracking users' posting activities in online classified ads and understanding the dynamics of their behavior is a topic of great importance with many implications. However, some of the underlying problems associated with modeling users and detecting their behavioral changes due to temporal and spatial variations have not been well-studied. In this paper, we develop a probabilistic model of user behavior based on the ads the user posts and the categories in which the ads are posted. The model can track some of the temporal changes in behavior, as revealed by our experiments on two classes of users monitored over a period of almost a year. We study the association between post categories and user groups, and show how temporal and seasonal changes can be detected. We further investigate a generative model for ad posts, based on user locations, and provide some evidence showing that the model is promising and that some interesting relationships can be identified.},
author = {Waqar, Muhammad and Rafiei, Davood},
doi = {10.1109/WI.2016.0088},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Tracking User Activities and Marketplace Dynamics in Classified Ads{\_}Waqar, Rafiei{\_}2017.pdf:pdf},
isbn = {9781509044702},
journal = {Proceedings - 2016 IEEE/WIC/ACM International Conference on Web Intelligence, WI 2016},
keywords = {Classified ads,Temporal analysis,User modeling,User tracking},
pages = {522--525},
publisher = {IEEE},
title = {{Tracking User Activities and Marketplace Dynamics in Classified Ads}},
year = {2017}
}
@article{Gurumdimma2016,
abstract = {{\textcopyright} 2016 IEEE. The use of console logs for error detection in large scale distributed systems has proven to be useful to system administrators. However, such logs are typically redundant and incomplete, making accurate detection very difficult. In an attempt to increase this accuracy, we complement these incomplete console logs with resource usage data, which captures the resource utilisation of every job in the system. We then develop a novel error detection methodology, the CRUDE approach, that makes use of both the resource usage data and console logs. We thus make the following specific technical contributions: we develop (i) a clustering algorithm to group nodes with similar behaviour, (ii) an anomaly detection algorithm to identify jobs with anomalous resource usage, (iii) an algorithm that links jobs with anomalous resource usage with erroneous nodes. We then evaluate our approach using console logs and resource usage data from the Ranger Supercomputer. Our results are positive: (i) our approach detects errors with a true positive rate of about 80{\%}, and (ii) when compared with the well-known Nodeinfo error detection algorithm, our algorithm provides an average improvement of around 85{\%} over Nodeinfo, with a best-case improvement of 250{\%}.},
author = {Gurumdimma, Nentawe and Jhumka, Arshad and Liakata, Maria and Chuah, Edward and Browne, James},
doi = {10.1109/SRDS.2016.017},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/CRUDE Combining Resource Usage Data and Error Logs for Accurate Error Detection in Large-Scale Distributed Systems{\_}Gurumdimma et al.{\_}201.pdf:pdf},
isbn = {9781509035137},
issn = {10609857},
journal = {Proceedings of the IEEE Symposium on Reliable Distributed Systems},
keywords = {anomaly detection,detection,event logs,faults,large-scale HPC systems,resource usage data,unsupervised},
pages = {51--60},
publisher = {IEEE},
title = {{CRUDE: Combining Resource Usage Data and Error Logs for Accurate Error Detection in Large-Scale Distributed Systems}},
year = {2016}
}
@article{ISO/IEC/IEEE©Std.4201:20112011,
author = {{ISO/IEC/IEEE{\textcopyright} Std. 4201:2011}},
doi = {10.1109/IEEESTD.2012.6170923},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/INTERNATIONAL STANDARD ISO IEC IEEE Systems and software engineering — agile environment{\_}ISOIECIEEE{\textcopyright} Std. 42012011{\_}2011.pdf:pdf},
journal = {ISO/IEC/IEEE 26515 First edition 2011-12-01; Corrected version 2012-03-15},
title = {{INTERNATIONAL STANDARD ISO / IEC / IEEE Systems and software engineering — agile environment}},
volume = {2012},
year = {2011}
}
@article{Syer2013,
abstract = {Load tests ensure that software systems are able to perform under the expected workloads. The current state of load test analysis requires significant manual review of performance counters and execution logs, and a high degree of system-specific expertise. In particular, memory-related issues (e.g., memory leaks or spikes), which may degrade performance and cause crashes, are difficult to diagnose. Performance analysts must correlate hundreds of megabytes or gigabytes of performance counters (to understand resource usage) with execution logs (to understand system behaviour). However, little work has been done to combine these two types of information to assist performance analysts in their diagnosis. We propose an automated approach that combines performance counters and execution logs to diagnose memory-related issues in load tests. We perform three case studies on two systems: one open-source system and one large-scale enterprise system. Our approach flags {\textless}0.1{\%} of the execution logs with a precision {\textgreater}80{\%}. {\textcopyright} 2013 IEEE.},
author = {Syer, Mark D. and Jiang, Zhen Ming and Nagappan, Meiyappan and Hassan, Ahmed E. and Nasser, Mohamed and Flora, Parminder},
doi = {10.1109/ICSM.2013.22},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Leveraging performance counters and execution logs to diagnose memory-related performance issues{\_}Syer et al.{\_}2013.pdf:pdf},
journal = {IEEE International Conference on Software Maintenance, ICSM},
keywords = {Execution Logs,Load Testing,Performance Counters,Performance Engineering},
pages = {110--119},
publisher = {IEEE},
title = {{Leveraging performance counters and execution logs to diagnose memory-related performance issues}},
year = {2013}
}
@article{Potey2013,
abstract = {Query log is the pouch of valuable information that records user's search queries and related actions on the internet. By mining the recorded information, it is possible to exploit the user's underlying goals, preferences, interests, search behaviors and implicit feedback. The wealth of mined information can be used in many applications such as query log analysis, query recommendation, query reformulation, query intent identification and many more to improve performance of search engine by providing more relevant results. Over the past decade, there has been tremendous work done for improving search engine results to flourish the users for searching. This paper reviews and compares some of the available methods to give an insight into the area of query log processing for information retrieval. Our approach classifies web query intent based on knowledge extraction from query log analysis. {\textcopyright} 2013 IEEE.},
author = {Potey, Madhuri A. and Patel, Dhanashri A. and Sinha, P. K.},
doi = {10.1109/IAdCC.2013.6514421},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/A survey of query log processing techniques and evaluation of web query intent identification{\_}Potey, Patel, Sinha{\_}2013.pdf:pdf},
isbn = {9781467345286},
journal = {Proceedings of the 2013 3rd IEEE International Advance Computing Conference, IACC 2013},
keywords = {Query Chains,Query Intent Identification,Query Log Processing,Query Recommendation,Query Reformulation},
pages = {1330--1335},
publisher = {IEEE},
title = {{A survey of query log processing techniques and evaluation of web query intent identification}},
year = {2013}
}
@inproceedings{Atterer2006,
abstract = {In this paper, we investigate how detailed tracking of user interaction can be monitored using standard web technologies. Our motivation is to enable implicit interaction and to ease usability evaluation of web applications outside the lab. To obtain meaningful statements on how users interact with a web application, the collected information needs to be more detailed and fine-grained than that provided by classical log files. We focus on tasks such as classifying the user with regard to computer usage proficiency or making a detailed assessment of how long it took users to fill in fields of a form. Additionally, it is important in the context of our work that usage tracking should not alter the user's experience and that it should work with existing server and browser setups. We present an implementation for detailed tracking of user actions on web pages. An HTTP proxy modifies HTML pages by adding JavaScript code before delivering them to the client. This JavaScript tracking code collects data about mouse movements, keyboard input and more. We demonstrate the usefulness of our approach in a case study.},
author = {Atterer, Richard and Wnuk, Monika and Schmidt, Albrecht},
booktitle = {Proceedings of the 15th International Conference on World Wide Web},
doi = {10.1145/1135777.1135811},
isbn = {1595933239},
keywords = {HTTP proxy,Implicit interaction,Mouse tracking,User activity tracking,Website usability evaluation},
title = {{Knowing the user's every move: User activity tracking for website usability evaluation and implicit interaction}},
year = {2006}
}
@article{Ackermann2009,
abstract = {In this paper, we analyze software that we inherited from another party. We analyze its architecture and use common design principles to identify critical changes in order to improve its flexibility with respect to a set of planned extensions. We describe flexibility issues that we encountered and how they were addressed by a redesign and re-implementation. The study shows that basic and well-established design concepts can be used to guide the design and redesign of software. {\textcopyright} 2009 IEEE.},
author = {Ackermann, Christopher and Lindvall, Mikael and Dennis, Greg},
doi = {10.1109/CSMR.2009.60},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Redesign for flexibility and maintainability a case study{\_}Ackermann, Lindvall, Dennis{\_}2009.pdf:pdf},
isbn = {9780769535890},
issn = {15345351},
journal = {Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR},
pages = {259--262},
publisher = {IEEE},
title = {{Redesign for flexibility and maintainability: a case study}},
year = {2009}
}
@article{Dwyer2013,
abstract = {Security is one of the biggest concerns of any company that has an IT infrastructure. Windows event logs are a very useful source of data for security information, but sometimes can be nearly impossible to use due to the complexity of log data or the number of events generated per minute. For this reason, event log data must be automatically processed so that an administrator is given a list of events that actually need the administrator's attention. This has been standard in intrusion detection systems for many years to find anomalies in network traffic, but has not been common in event log processing. This paper will adapt these intrusion detection techniques for Windows event log data sets to find anomalies in these log data sets. ?? 2013 ICST.},
author = {Dwyer, John and Truta, Traian Marius},
doi = {10.4108/icst.collaboratecom.2013.254136},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Finding anomalies in windows event logs using standard deviation{\_}Dwyer, Truta{\_}2013.pdf:pdf},
isbn = {9781936968923},
journal = {Proceedings of the 9th IEEE International Conference on Collaborative Computing: Networking, Applications and Worksharing, COLLABORATECOM 2013},
keywords = {Anomaly Detection,Standard Deviation,Windows Event Logs},
pages = {563--570},
title = {{Finding anomalies in windows event logs using standard deviation}},
year = {2013}
}
@article{Niu2018,
abstract = {This article summarizes the RE in the Age of Continuous Deployment panel at the 25th IEEE International Requirements Engineering Conference. It highlights two synergistic points (user stories and linguistic tooling) and one challenge (nonfunctional requirements) in fast-paced, agile-like projects, and recommends how to carry on the dialogue.},
author = {Niu, Nan and Brinkkemper, Sjaak and Franch, Xavier and Partanen, Jari and Savolainen, Juha},
doi = {10.1109/MS.2018.1661332},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Requirements engineering and continuous deployment{\_}Niu et al.{\_}2018.pdf:pdf},
issn = {07407459},
journal = {IEEE Software},
keywords = {25th IEEE International Requirements Engineering C,RE in the Age of Continuous Deployment,agile software development,continuous deployment,linguistic tooling,nonfunctional requirements,requirements engineering,software development,software engineering,software requirements,user stories},
number = {2},
pages = {86--90},
publisher = {IEEE},
title = {{Requirements engineering and continuous deployment}},
volume = {35},
year = {2018}
}
@article{Lei2018,
author = {Lei, Xuezhi},
doi = {10.1109/ESAIC.2018.00052},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Modeling and intelligent analysis of web user behavior of web user behavior{\_}Lei{\_}2018.pdf:pdf},
isbn = {9781538680285},
journal = {Proceedings - 2018 International Conference on Engineering Simulation and Intelligent Control, ESAIC 2018},
keywords = {Data mining,Intelligent analysis,Interest degree,User behavior},
pages = {192--195},
title = {{Modeling and intelligent analysis of web user behavior of web user behavior}},
year = {2018}
}
@article{Stojanov2017,
abstract = {Software maintenance has been recognized by academicians and practitioners from industry as the most challenging and expensive part in software life cycle. The complexity and high costs of maintenance activities require systematic evidence of all maintenance activities and accurate models for planning and managing them. A common way for analyzing practice in software engineering is based on trend analysis of historical data related to activities and tasks implemented in the past. This paper presents a case study conducted in a micro software company aimed at introducing a schema for classifying maintenance tasks, and identifying trends in software maintenance tasks distribution among the programmers in the company. The discussion of results includes benefits for the company, limitations of the research and implications for academicians and practitioners in industry. The paper concludes with a few promising further research directions.},
author = {Stojanov, Zeljko and Stojanov, Jelena and Dobrilovic, Dalibor and Petrov, Nikola},
doi = {10.1109/SISY.2017.8080547},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Trends in software maintenance tasks distribution among programmers A study in a micro software company{\_}Stojanov et al.{\_}2017.pdf:pdf},
isbn = {9781538638552},
journal = {SISY 2017 - IEEE 15th International Symposium on Intelligent Systems and Informatics, Proceedings},
pages = {23--27},
title = {{Trends in software maintenance tasks distribution among programmers: A study in a micro software company}},
year = {2017}
}
@article{EvangelinGeetha2007,
abstract = {Software performance is an important nonfunctional attribute of software systems for producing quality software. Performance issues must be considered throughout software project development. Predicting performance early in the life cycle is addressed by many methodologies, but the data collected during feasibility study not considered for predicting performance. In this paper, we consider the data collected (technical and environmental factors) during feasibility study of software project management to predict performance. We derive an algorithm to predict the performance metrics and simulate the results using a case study on banking application. {\textcopyright}2007 IEEE.},
author = {{Evangelin Geetha}, D. and {Suresh Kumar}, T. V. and {Rajani Kanth}, K.},
doi = {10.1109/ICICS.2007.4449845},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Predicting performance of software systems during feasibility study of software project management{\_}Evangelin Geetha, Suresh Kumar, Rajan.pdf:pdf},
isbn = {1424409837},
journal = {2007 6th International Conference on Information, Communications and Signal Processing, ICICS},
keywords = {Feasibility study,Software performance engineering,Use case point},
pages = {1--5},
title = {{Predicting performance of software systems during feasibility study of software project management}},
year = {2007}
}
@article{Song2008,
abstract = {Although information systems support a wide range of recreational and social activities, the pattern of users' behavior in such systems is not clear. We extend the process mining technology to work on common event logs that have no workflow cases reference, and name the new technology as behavior pattern mining. This paper gives out a brief survey on issues, challenges, approaches and related tools in behavior pattern mining area, and compares behavior pattern mining with workflow mining technology, which is the other sub field of process mining. To reply the problems proposed by behavior pattern mining, the process mining area gets its new motivation.},
author = {Song, Jinliang and Luo, Tiejian and Chen, Su},
doi = {10.1109/ICNSC.2008.4525516},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Behavior pattern mining Apply process mining technology to common event logs of information systems{\_}Song, Luo, Chen{\_}2008.pdf:pdf},
isbn = {9781424416851},
journal = {Proceedings of 2008 IEEE International Conference on Networking, Sensing and Control, ICNSC},
pages = {1800--1805},
title = {{Behavior pattern mining: Apply process mining technology to common event logs of information systems}},
year = {2008}
}
@article{Bozhikova2017,
abstract = {The term "quality software" refers to software that is easy to maintain and evolve. The presence of Anti-Patterns and Patterns is recognized as one of the effective ways to measure the quality of modern software systems. The paper presents an approach which supports the software analysis, development and maintenance, using techniques that generate the structure of Software Design Patterns, find Anti-Patterns in the code and perform Code Refactoring. The proposed approach is implemented in a software tool, which could support the real phases of software development and could be used for educational purposes, to support "Advanced Software Engineering" course.},
author = {Bozhikova, Violeta and Stoeva, Mariana and Georgiev, Bozhidar and Nikolaeva, Dimitrichka},
doi = {10.1109/ET.2017.8124337},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Improving the software quality - An educational approach{\_}Bozhikova et al.{\_}2017.pdf:pdf},
isbn = {9781538617533},
journal = {2017 26th International Scientific Conference Electronics, ET 2017 - Proceedings},
keywords = {Software anti-patterns,Software design patterns,Software engineering,Software refactoring},
pages = {1--4},
title = {{Improving the software quality - An educational approach}},
volume = {2017-Janua},
year = {2017}
}
@article{Baccanico2014,
abstract = {This paper discusses our preliminary analysis of event logging practices adopted in a large-scale industrial development process at Selex ES, a top-leading Finmeccanica company in electronic and information technologies for defense systems, aerospace, and land security. The analysis aims to support log reengineering activities that are currently conducted at SELEX ES. At time being, some of the issues described in the paper have been fixed by system developers. Analysis encompasses total around 50+ millions lines of log produced by an Air Traffic Control (ATC) system. Analysis reveals that event logging is not strictly regulated by company-wide practices, which results into heterogeneous logs across different development teams. We introduce our ongoing effort at developing an automatic support to browse collected logs along with a uniform logging policy supplementing the reengineering process.},
author = {Baccanico, Fabio and Carrozza, Gabriella and Cinque, Marcello and Cotroneo, Domenico and Pecchia, Antonio and Savignano, Agostino},
doi = {10.1109/ISSREW.2014.69},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Event logging in an industrial development process Practices and reengineering challenges{\_}Baccanico et al.{\_}2014.pdf:pdf},
isbn = {9781479973774},
journal = {Proceedings - IEEE 25th International Symposium on Software Reliability Engineering Workshops, ISSREW 2014},
keywords = {Air traffic control,Development process,Event logging,Logging practices},
number = {i},
pages = {10--13},
publisher = {IEEE},
title = {{Event logging in an industrial development process: Practices and reengineering challenges}},
year = {2014}
}
@article{Vijayasarathy2016,
abstract = {Organizations can choose from software development methodologies ranging from traditional to agile approaches. Researchers surveyed project managers and other team members about their choice of methodologies. The results indicate that although agile methodologies such as Agile Unified Process and Scrum are more prevalent than 10 years ago, traditional methodologies, including the waterfall model, are still popular. Organizations are also taking a hybrid approach, using multiple methodologies on projects. Furthermore, their choice of methodologies is associated with certain organizational, project, and team characteristics.},
author = {Vijayasarathy, Leo R. and Butler, Charles W.},
doi = {10.1109/MS.2015.26},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Choice of Software Development Methodologies Do Organizational, Project, and Team Characteristics Matter{\_}Vijayasarathy, Butler{\_}2016.pdf:pdf},
issn = {07407459},
journal = {IEEE Software},
keywords = {agile methodologies,hybrid software development,organizational characteristics,project characteristics,software development,software development methodologies,software engineering,team characteristics,traditional methodologies,waterfall model},
number = {5},
pages = {86--94},
publisher = {IEEE},
title = {{Choice of Software Development Methodologies: Do Organizational, Project, and Team Characteristics Matter?}},
volume = {33},
year = {2016}
}
@article{Stark1996,
abstract = {Software maintenance is central to the mission of many organizations. Thus, it is natural for managers to characterize and measure those aspects of products and processes that seem to affect cost, schedule, quality, and functionality of a software maintenance delivery. This paper answers basic questions about software maintenance for a single organization and discusses some of the decisions made based on the answers. Attributes of both the software maintenance process and the resulting product were measured to direct management and engineering attention toward improvement areas, track the improvement over time, and help make choices among alternatives.},
author = {Stark, George E.},
doi = {10.1109/icsm.1996.565000},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Measurements for managing software maintenance{\_}Stark{\_}1996.pdf:pdf},
journal = {Conference on Software Maintenance},
pages = {152--161},
title = {{Measurements for managing software maintenance}},
year = {1996}
}
@article{Niazi2016,
abstract = {Context Global Software Development (GSD) is the process whereby software is developed by different teams located in various parts of the globe. One of the major reasons for GSD project failure is that a number of organizations endorse global development prior to understanding project management challenges for the global activity. Objective The objective of this paper is to identify the challenges, from the client and vendor perspectives, which can undermine the successful management of GSD projects. Method We followed a two-phase approach: we first identified the challenges via a Systematic Literature Review (SLR) and then the identified challenges were validated using a questionnaire-based survey. Results Through both approaches, we identified 19 challenges important to the success of GSD project management. A comparison of the challenges identified in client and vendor organizations indicates that there are more similarities than differences between the challenges. Our results show a positive correlation between the ranks obtained from the SLR and the questionnaire ((rs(19) = 0.102), p = 0.679). The results of t-test (i.e., t = 0.299, p = 0.768{\textgreater}0.05) show that there is no significant difference between the findings of SLR and questionnaire. Conclusions GSD organizations should try to address the identified challenges when managing their global software development activities to increase the probability of project success.},
author = {Niazi, Mahmood and Mahmood, Sajjad and Alshayeb, Mohammad and Riaz, Mohammed Rehan and Faisal, Kanaan and Cerpa, Narciso and Khan, Siffat Ullah and Richardson, Ita},
doi = {https://doi.org/10.1016/j.infsof.2016.08.002},
issn = {0950-5849},
journal = {Information and Software Technology},
keywords = {Challenges and barriers,Empirical study,Global software development,Software project management,Systematic literature review},
pages = {1--19},
title = {{Challenges of project management in global software development: A client-vendor analysis}},
url = {http://www.sciencedirect.com/science/article/pii/S0950584916301227},
volume = {80},
year = {2016}
}
@article{Thankachan2018,
abstract = {This paper discusses the design of a solution to enable data driven decision making for application support. The design proposes a novel standard for logging within applications. The next phase of the design proposes a novel method to use the standardized logs to map the functioning of an application to a finite state automata. The analytics proposed in the design helps understand issues during application processing, assess the impact of issues and quickly take decisions to resolve the issues. Big data analytics and probabilistic models are used on the historical application logs to further predict issues prior to their occurrence, assess the health of application functioning and be able to proactively act on situations that can lead to errors.},
author = {Thankachan, Karun},
doi = {10.1109/ICICI.2017.8365229},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Data driven decision making for application support{\_}Thankachan{\_}2018.pdf:pdf},
isbn = {9781538640319},
journal = {Proceedings of the International Conference on Inventive Computing and Informatics, ICICI 2017},
keywords = {Big data analytics,Finite state automata,Machine learning,Prescriptive systems,Probabilistic models},
number = {Icici},
pages = {716--720},
title = {{Data driven decision making for application support}},
year = {2018}
}
@article{Vaarandi2015,
abstract = {Modern IT systems often produce large volumes of event logs, and event pattern discovery is an important log management task. For this purpose, data mining methods have been suggested in many previous works. In this paper, we present the LogCluster algorithm which implements data clustering and line pattern mining for textual event logs. The paper also describes an open source implementation of LogCluster.},
author = {Vaarandi, Risto and Pihelgas, Mauno},
doi = {10.1109/CNSM.2015.7367331},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/LogCluster - A data clustering and pattern mining algorithm for event logs{\_}Vaarandi, Pihelgas{\_}2015.pdf:pdf},
isbn = {9783901882777},
journal = {Proceedings of the 11th International Conference on Network and Service Management, CNSM 2015},
keywords = {data clustering,data mining,event log analysis,event log clustering,mining patterns from event logs},
pages = {1--7},
title = {{LogCluster - A data clustering and pattern mining algorithm for event logs}},
year = {2015}
}
@article{Charette1997,
abstract = {Risk management in maintenance differs in major ways from risk management in development. Risk opportunities are more frequent, risks come from more diverse sources, and projects have less freedom to act on them. The authors describe how they dealt with these differences in a large US Navy software maintenance organization.},
author = {Charette, Robert N. and Adams, Kevin Mac G. and White, Mary B.},
doi = {10.1109/52.589232},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Managing risk in software maintenance{\_}Charette, Adams, White{\_}1997.pdf:pdf},
issn = {07407459},
journal = {IEEE Software},
number = {3},
pages = {43--50},
title = {{Managing risk in software maintenance}},
volume = {14},
year = {1997}
}
@article{Booch1986,
abstract = {Object-oriented development is a partial-Iifecycle software development method in which the decomposition of a system is based upon the concept of an object. This method is fundamentally different from traditional functional approaches to design and serves to help manage the complexity of massive software-intensive systems. The pa- per examines the process of object-oriented development as well as the influences upon this approach from advances in abstraction mecha- nisms, programming languages, and hardware. The concept of an ob- ject is central to object-oriented development and so the properties of an object are discussed in detail. The paper concludes with an exami- nation of the mapping of object-oriented techniques to Ada" using a design case study},
author = {Booch, Grady},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Object-Oriented Development{\_}Booch{\_}1986.pdf:pdf},
journal = {IEEE Transactions on Software Engineering},
keywords = {Abstract data type,Ada,object,object-oriented de- velopment,software development method.},
mendeley-tags = {Abstract data type,Ada,object,object-oriented de- velopment,software development method.},
number = {2},
pages = {211--221},
publisher = {IEEE},
title = {{Object-Oriented Development}},
volume = {SE-12},
year = {1986}
}
@article{Hasan2012,
abstract = {Software maintenance constitutes a critical function that enables organizations to continually leverage their information technology (IT) capabilities. Despite the growing importance of small organizations, a majority of the software maintenance guidelines are inherently geared toward large organizations. Literature review and case-based empirical studies show that in small organizations software maintenance processes are carried out without following a systemic process. Rather, they rely on ad-hoc and heuristics methods by organizations and individuals. This paper investigates software maintenance practices in a small information systems organization to come up with the nature and categories of heuristics used that successfully guided the software maintenance process. Specifically, this paper documents a set of best practices that small organizations can adopt to facilitate their software maintenance processes in the absence of maintenance-specific guidelines based on preliminary empirical investigation.},
author = {Hasan, Raza and Chakraborty, Suranjan and Dehlinger, Josh},
doi = {10.1007/978-3-642-23202-2-9},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Examining software maintenance processes in small organizations Findings from a case study{\_}Hasan, Chakraborty, Dehlinger{\_}2012.pdf:pdf},
isbn = {9783642232015},
issn = {1860949X},
journal = {Studies in Computational Intelligence},
keywords = {Small organizations,ad-hoc process,case study,cognitive heuristics,software maintenance},
pages = {129--143},
title = {{Examining software maintenance processes in small organizations: Findings from a case study}},
volume = {377},
year = {2012}
}
@article{Pathan2014,
abstract = {Data mining is the process of finding correlations in the relational databases. There are different techniques for identifying malicious database transactions. Many existing approaches which profile is SQL query structures and database user activities to detect intrusion, the log mining approach is the automatic discovery for identifying anomalous database transactions. Mining of the Data is very helpful to end users for extracting useful business information from large database. Multi-level and multi-dimensional data mining are employed to discover data item dependency rules, data sequence rules, domain dependency rules, and domain sequence rules from the database log containing legitimate transactions. Database transactions that do not comply with the rules are identified as malicious transactions. The log mining approach can achieve desired true and false positive rates when the confidence and support are set up appropriately. The implemented system incrementally maintain the data dependency rule sets and optimize the performance of the intrusion detection process. {\textcopyright} 2014 IEEE.},
author = {Pathan, Apashabi Chandkhan and Potey, Madhuri A.},
doi = {10.1109/ICESC.2014.50},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Detection of malicious transaction in database using log mining approach{\_}Pathan, Potey{\_}2014.pdf:pdf},
isbn = {9781479921027},
journal = {Proceedings - International Conference on Electronic Systems, Signal Processing, and Computing Technologies, ICESC 2014},
keywords = {Data Mining,Database security,Intrusion Detection},
pages = {262--265},
publisher = {IEEE},
title = {{Detection of malicious transaction in database using log mining approach}},
year = {2014}
}
@article{Tang2010,
abstract = {As the information technology industry gains maturity, the number of software systems having moved into maintenance is rapidly growing. Software maintenance is a costly, yet often neglected part of the development life-cycle. A software product maybe has been modified several times for different reasons, but this process don't be efficient manage, so resulting in the software been discard in advance. This paper is motivated by a desire to develop a more practical model to track and manage the maintenance process, to support the maintenance task. The maintenance request form (MRF)'s submission means the start of maintenance activities. One of important tasks is tracking the state of MRF, to control activities in the current environment of large and complex applications. And the other character is import measure into the model, to offer information to help organization control and processing their activities. {\textcopyright} 2010 IEEE.},
author = {Tang, Li and Mei, Yong Gang and Ding, Jian Jie},
doi = {10.1109/ETCS.2010.571},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Metric-based tracking management in software maintenance{\_}Tang, Mei, Ding{\_}2010.pdf:pdf},
isbn = {9780769539874},
journal = {2nd International Workshop on Education Technology and Computer Science, ETCS 2010},
keywords = {Metric,Software maintenancet,Tracking management},
pages = {675--678},
publisher = {IEEE},
title = {{Metric-based tracking management in software maintenance}},
volume = {1},
year = {2010}
}
@article{Sosnowski2014,
author = {Sosnowski, J and Gawkowski, P and Cabaj, K},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Event and performance logs in system management and evaluation{\_}Sosnowski, Gawkowski, Cabaj{\_}2014.pdf:pdf},
journal = {Information Systems in {\ldots}},
number = {September},
title = {{Event and performance logs in system management and evaluation}},
year = {2014}
}
@article{Alenezi2016,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 296060207 Does Software ? Evidences - Source Article CITATIONS 0 READS 38 2 : Some : Develop (UX) Evaluation Architectural - Source Mamdouh Prince 32 SEE Mohammad . Zarour Prince 32 SEE All . Zarour . The . Abstract Throughout the software evolution , several maintenance actions such as adding new fea - tures , fixing problems , improving the design might negatively or positively affect the software design quality . Quality degradation , if not handled in the right time , can accumulate and cause serious problems for future maintenance effort . Several researchers considered modu - larity as one of the success factors of Open Source Software (OSS) Projects . The modularity of these systems is influenced by some software metrics such as size , complexity , cohesion , and coupling . In this work , we study the modularity evolution of four open - source systems by answering two main research questions namely : what measures can be used to measure the modularity level of software and secondly , did the modularity level for the selected open source software improves over time . By investigating the modularity measures , we have identified the main measures that can be used to measure software modularity . Based on our analysis , the modularity of these two systems is not improving over time . However , the defect density is improving over time .},
author = {Alenezi, Mamdouh and Zarour, Mohammad},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Does Software Structures Quality Improve over Software Evolution Evidences from Open - Source Projects{\_}Alenezi, Zarour{\_}2016.pdf:pdf},
journal = {Special issue on “Computing Applications and Data Mining” International Journal of Computer Science and Information Security (IJCSIS)},
number = {1},
pages = {61--75},
title = {{Does Software Structures Quality Improve over Software Evolution? Evidences from Open - Source Projects}},
volume = {14},
year = {2016}
}
@article{Dhanalakshmi2016,
abstract = {The increased on-line applications are leading to exponential growth of the web content. Most of the business organizations are interested to know the web user behavior to enhance their business. In this context, users navigation in static and dynamic web applications plays an important role in understanding user's interests. The static mining techniques may not be suitable as it is for dynamic web log files and decision making. Traditional web log preprocessing approaches and weblog usage patterns have limitations to analyze the content relationship with the browsing history This paper, focuses on various static web log preprocessing and mining techniques and their applicable limitations for dynamic web mining.},
author = {Dhanalakshmi, P. and Ramani, K. and Reddy, B. Eswara},
doi = {10.1109/IACC.2016.35},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/The Research of Preprocessing and Pattern Discovery Techniques on Web Log Files{\_}Dhanalakshmi, Ramani, Reddy{\_}2016.pdf:pdf},
isbn = {9781467382861},
journal = {Proceedings - 6th International Advanced Computing Conference, IACC 2016},
keywords = {Asscociation rules,Graph models,Navigation patterns,Static Logs,Web log},
pages = {139--145},
publisher = {IEEE},
title = {{The Research of Preprocessing and Pattern Discovery Techniques on Web Log Files}},
year = {2016}
}
@article{Bekeneva2020,
abstract = {In recent years, process mining algorithms are widely used for process analysis. As input data for process mining algorithms,.xes files are used. This format has a limitation for a number of attributes; therefore, in case of registering a single event with several monitoring devices, there is problem of generating event logs based on heterogeneous data. In this paper, an algorithm for generating event logs based on data from heterogeneous monitoring devices is proposed. The most important parameters for the analysis of events are taken into account. Examples of the formation of event logs when choosing a different set of source data are given, the influence of the number and composition of the selected attributes on the result of building business process models is analyzed.},
author = {Bekeneva, Yana A.},
doi = {10.1109/EIConRus49466.2020.9039350},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Algorithm for Generating Event Logs Based on Data from Heterogeneous Sources{\_}Bekeneva{\_}2020.pdf:pdf},
isbn = {9781728157610},
journal = {Proceedings of the 2020 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering, EIConRus 2020},
keywords = {.xes files,event logs,log files,process mining},
pages = {233--236},
publisher = {IEEE},
title = {{Algorithm for Generating Event Logs Based on Data from Heterogeneous Sources}},
year = {2020}
}
@article{Lenarduzzi2017,
abstract = {Software maintenance has dramatically evolved in the last four decades, to cope with the continuously changing software development models, and programming languages and adopting increasingly advanced prediction models. In this work, we present the initial results of a Systematic Literature Review (SLR), highlighting the evolution of the metrics and models adopted in the last forty years.},
author = {Lenarduzzi, Valentina and Sillitti, Alberto and Taibi, Davide},
doi = {10.1109/ICSE-C.2017.122},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Analyzing Forty years of software maintenance models{\_}Lenarduzzi, Sillitti, Taibi{\_}2017.pdf:pdf},
isbn = {9781538615898},
journal = {Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017},
keywords = {Software Maintenance,Systematic Literature Review},
pages = {146--148},
publisher = {IEEE},
title = {{Analyzing Forty years of software maintenance models}},
year = {2017}
}
@article{Galster2019,
author = {Galster, Matthias and Treude, Christoph and Blincoe, Kelly},
doi = {10.1109/ICSME.2019.00060},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Supporting Software Architecture Maintenance by Providing Task-specific Recommendations{\_}Galster, Treude, Blincoe{\_}2019.pdf:pdf},
journal = {2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
pages = {370--372},
publisher = {IEEE},
title = {{Supporting Software Architecture Maintenance by Providing Task-specific Recommendations}},
year = {2019}
}
@inproceedings{Chen2019,
author = {Chen, Boyuan},
doi = {10.1109/icse-companion.2019.00080},
pages = {194--197},
title = {{Improving the Software Logging Practices in DevOps}},
year = {2019}
}
@inproceedings{Mueller2001,
abstract = {Conventional web interfaces respond to and consider only mouse clicks when defining a user model. We have extended this and take into account all mouse movements on a page as an additional layer of information for inferring user interest. We have developed a straightforward way to record all mouse movements on a page, and conducted a user study to analyze and investigate mouse behavior trends. We found certain mouse behaviors, common across many users, which are useful for content providers in increasing the effectiveness of their interface design. Copyright {\textcopyright} 2012 ACM, Inc.},
author = {Mueller, Florian and Lockerd, Andrea},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/634067.634233},
isbn = {1581133405},
keywords = {Context,Mouse activity,Mouse movements,User study,Web mouse tracking design guidelines,Web page design},
title = {{Cheese: Tracking mouse movement activity on websites, a tool for user modeling}},
year = {2001}
}
@article{Lu2019,
abstract = {A decision problem, according to traditional principles, is approached by finding an optimal solution to an analytical programming decision model, which is known as model-driven decision-making. The fidelity of the model determines the quality and reliability of the decision-making; however, the intrinsic complexity of many real-world decision problems leads to significant model mismatch or infeasibility in deriving a model using the first principle. To overcome the challenges that are present in the big data era, both researchers and practitioners emphasize the importance of making decisions that are backed up by data related to decision tasks, a process called data-driven decision-making (D3M). By building on data science, not only can decision models be predicted in the presence of uncertainty or unknown dynamics, but also inherent rules or knowledge can be extracted from data and directly utilized to generate decision solutions. This position paper systematically discusses the basic concepts and prevailing techniques in data-driven decision-making and clusters-related developments in technique into two main categories: programmable data-driven decision-making (P-D3M) and nonprogrammable data-driven decision-making (NP-D3M). This paper establishes a D3M technical framework, main methodologies, and approaches for both categories of D3M, as well as identifies potential methods and procedures for using data to support decision-making. It also provides examples of how D3M is implemented in practice and identifies five further research directions in the D3M area. We believe that this paper will directly support researchers and professionals in their understanding of the fundamentals of D3M and of the developments in technical methods},
author = {Lu, Jie and Yan, Zheng and Han, Jialin and Zhang, Guangquan},
doi = {10.1109/tetci.2019.2915813},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Data-Driven Decision-Making (D 3 M) Framework, Methodology, and Directions{\_}Lu et al.{\_}2019.pdf:pdf},
journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
number = {4},
pages = {286--296},
publisher = {IEEE},
title = {{ Data-Driven Decision-Making (D 3 M): Framework, Methodology, and Directions }},
volume = {3},
year = {2019}
}
@article{Cui2003,
abstract = {Queries to search engines on the Web are usually short. They do not provide sufficient information for an effective selection of relevant documents. Previous research has proposed the utilization of query expansion to deal with this problem. However, expansion terms are usually determined on term co-occurrences within documents. In this study, we propose a new method for query expansion based on user interactions recorded in user logs. The central idea is to extract correlations between query terms and document terms by analyzing user logs. These correlations are then used to select high-quality expansion terms for new queries. Compared to previous query expansion methods, ours takes advantage of the user judgments implied in user logs. The experimental results show that the log-based query expansion method can produce much better results than both the classical search method and the other query expansion methods.},
author = {Cui, Hang and Wen, Ji Rong and Nie, Jian Yun and Ma, Wei Ying},
doi = {10.1109/TKDE.2003.1209002},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Query expansion by mining user logs{\_}Cui et al.{\_}2003.pdf:pdf},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Information retrieval,Probabilistic model,Query expansion,Search engine,User log},
number = {4},
pages = {829--839},
title = {{Query expansion by mining user logs}},
volume = {15},
year = {2003}
}
@article{Rehman2018,
abstract = {Agile methodologies gained fame due to the fact of producing high-quality software systems. Maintenance effort is almost more than half of the total effort invested in any software system during its lifespan. A well-discussed issue within the community of researchers and engineers is how to use agile methodologies for maintaining the developed software because agile software development life cycle doesn't have the specifically planned mechanism for maintenance. To bridge this gap, we used the theoretical and empirical technique to formulate factors that should be followed during the agile maintenance including planning for the maintenance; the on-site customer should be present, iterative maintenance, documentation update after each phase and maintenance should be testable.},
author = {Rehman, Fateh Ur and Maqbool, Bilal and Riaz, Muhammad Qasim and Qamar, Usman and Abbas, Muhammad},
doi = {10.1109/NCG.2018.8593152},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Scrum Software Maintenance Model Efficient Software Maintenance in Agile Methodology{\_}Rehman et al.{\_}2018.pdf:pdf},
isbn = {9781538641095},
journal = {21st Saudi Computer Society National Computer Conference, NCC 2018},
keywords = {Agile,Agile Maintenance,Maintenance Sprints,Scrum,Scrum Maintenance,Software Maintenance},
pages = {7--11},
publisher = {IEEE},
title = {{Scrum Software Maintenance Model: Efficient Software Maintenance in Agile Methodology}},
year = {2018}
}
@article{Port2017,
abstract = {NASA has been successfully sustaining the continuous operation of its critical navigation software systems for over 12 years. To accomplish this, NASA scientists must continuously monitor their process, report on current system quality, forecast maintenance effort, and sustain required staffing levels. This report presents some examples of the use of a robust software metrics and analytics program that enables actionable strategic maintenance management of a critical system (Monte) in a timely, economical, and risk-controlled fashion. This article is part of a special issue on Actionable Analytics for Software Engineering.},
author = {Port, Dan and Taber, Bill},
doi = {10.1109/MS.2017.4541055},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Actionable Analytics for Strategic Maintenance of Critical Software An Industry Experience Report{\_}Port, Taber{\_}2017.pdf:pdf},
issn = {07407459},
journal = {IEEE Software},
keywords = {Monte,NASA,critical systems,navigation systems,reliability,software analytics,software development,software engineering,software maintenance},
number = {1},
pages = {58--63},
publisher = {IEEE},
title = {{Actionable Analytics for Strategic Maintenance of Critical Software: An Industry Experience Report}},
volume = {35},
year = {2017}
}
@article{Razavi2008,
author = {Razavi, Ali and Kontogiannis, Kostas},
doi = {10.1109/COMPSAC.2008.81},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Pattern and policy driven log analysis for software monitoring{\_}Razavi, Kontogiannis{\_}2008.pdf:pdf},
isbn = {9780769532622},
issn = {07303157},
journal = {Proceedings - International Computer Software and Applications Conference},
pages = {108--111},
title = {{Pattern and policy driven log analysis for software monitoring}},
year = {2008}
}
@article{Sneed2004,
abstract = {The purpose of this essay is to present a costing model for software maintenance and evolution based on a separation of fixed and variable costs. There has always been a problem in distinguishing between the maintenance activities covered by the standard maintenance fee and those charged extra to the user. Separating these two types of costs is essential to every maintenance operation to prevent costs from getting out of control. In this paper the author proposes a solution, which can lead to better cost estimations and a financially more stable maintenance operation. Particular emphasis is placed on a sharp division between work done to maintain the system functionality as it is and work done to enhance that functionality. {\textcopyright} 2004 IEEE.},
author = {Sneed, Harry M.},
doi = {10.1109/ICSM.2004.1357810},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/A cost model for software maintenance {\&} evolution{\_}Sneed{\_}2004.pdf:pdf},
journal = {IEEE International Conference on Software Maintenance, ICSM},
keywords = {Maintenance Cost Estimation,Software Life Cycle Costing Models,Software Maintenance and Evolution,Software Product Management},
pages = {264--273},
publisher = {IEEE},
title = {{A cost model for software maintenance {\&} evolution}},
year = {2004}
}
@article{Snipes2018,
abstract = {In large-scale software systems, the majority of defective files are architecturally connected, and the architecture connections usually exhibit design flaws, which are associated with higher change-proneness among files and higher maintenance costs. As software evolves with bug fixes, new features, or improvements, unresolved architecture design flaws can contribute to maintenance difficulties. The impact on effort due to architecture design flaws has been difficult to quantify and justify. In this paper, we conducted a case study where we identified flawed architecture relations and quantified their effects on maintenance activities. Using data from this project's source code and revision history, we identified file groups where files are architecturally connected and participated in flawed architecture designs, quantified the maintenance activities in the detected files, and assessed the penalty related to these files.},
author = {Snipes, Will and Karlekar, Sunil L. and Mo, Ran},
doi = {10.1109/SEAA.2018.00071},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/A case study of the effects of architecture debt on software evolution effort{\_}Snipes, Karlekar, Mo{\_}2018.pdf:pdf},
isbn = {9781538673829},
journal = {Proceedings - 44th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2018},
keywords = {Software architecture,Software maintenance,Technical debt},
pages = {400--403},
publisher = {IEEE},
title = {{A case study of the effects of architecture debt on software evolution effort}},
year = {2018}
}
@article{Song2017,
abstract = {The aligning of event logs with process models is of great significance for process mining to enable conformance checking, process enhancement, performance analysis, and trace repairing. Since process models are increasingly complex and event logs may deviate from process models by exhibiting redundant, missing, and dislocated events, it is challenging to determine the optimal alignment for each event sequence in the log, as this problem is NP-hard. Existing approaches utilize the cost-based A∗ algorithm to address this problem. However, scalability is often not considered, which is especially important when dealing with industrial-sized problems. In this paper, by taking advantage of the structural and behavioral features of process models, we present an efficient approach which leverages effective heuristics and trace replaying to significantly reduce the overall search space for seeking the optimal alignment. We employ real-world business processes and their traces to evaluate the proposed approach. Experimental results demonstrate that our approach works well in most cases, and that it outperforms the state-of-the-art approach by up to 5 orders of magnitude in runtime efficiency. {\textcopyright} 2016 IEEE.},
author = {Song, Wei and Xia, Xiaoxu and Jacobsen, Hans Arno and Zhang, Pengcheng and Hu, Hao},
doi = {10.1109/TSC.2016.2601094},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Efficient Alignment Between Event Logs and Process Models{\_}Song et al.{\_}2017.pdf:pdf},
issn = {19391374},
journal = {IEEE Transactions on Services Computing},
keywords = {Event logs,alignment,process decomposition,process models,trace replaying,trace segmentation},
number = {1},
pages = {136--149},
publisher = {IEEE},
title = {{Efficient Alignment Between Event Logs and Process Models}},
volume = {10},
year = {2017}
}
@article{Park2017,
abstract = {Today's high-performance computing (HPC) systems are heavily instrumented, generating logs containing information about abnormal events, such as critical conditions, faults, errors and failures, system resource utilization, and about the resource usage of user applications. These logs, once fully analyzed and correlated, can produce detailed information about the system health, root causes of failures, and analyze an application's interactions with the system, providing valuable insights to domain scientists and system administrators. However, processing HPC logs requires a deep understanding of hardware and software components at multiple layers of the system stack. Moreover, most log data is unstructured and voluminous, making it more difficult for system users and administrators to manually inspect the data. With rapid increases in the scale and complexity of HPC systems, log data processing is becoming a big data challenge. This paper introduces a HPC log data analytics framework that is based on a distributed NoSQL database technology, which provides scalability and high availability, and the Apache Spark framework for rapid in-memory processing of the log data. The analytics framework enables the extraction of a range of information about the system so that system administrators and end users alike can obtain necessary insights for their specific needs. We describe our experience with using this framework to glean insights from the log data about system behavior from the Titan supercomputer at the Oak Ridge National Laboratory.},
author = {Park, Byung H. and Hukerikar, Saurabh and Adamson, Ryan and Engelmann, Christian},
doi = {10.1109/CLUSTER.2017.113},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Big data meets HPC log analytics Scalable approach to understanding systems at extreme scale{\_}Park et al.{\_}2017.pdf:pdf},
isbn = {9781538623268},
issn = {15525244},
journal = {Proceedings - IEEE International Conference on Cluster Computing, ICCC},
keywords = {Big data processing,Log data analytics,System monitoring},
pages = {758--765},
title = {{Big data meets HPC log analytics: Scalable approach to understanding systems at extreme scale}},
volume = {2017-Septe},
year = {2017}
}
@article{Rong2018,
abstract = {Background: Logs are the footprints that software systems produce during runtime, which can be used to understand the dynamic behavior of these software systems. To generate logs, logging practice is accepted by developers to place logging statements in the source code of software systems. Compared to the great number of studies on log analysis, the research on logging practice is relatively scarce, which raises a very critical question, i.e. as the original intention, can current logging practice support capturing the behavior of software systems effectively? Aims: To answer this question, we first need to understand how logging practices are implemented these software projects. Method: In this paper, we carried out an empirical study to explore the logging practice in open source software projects so as to establish a basic understanding on how logging practice is applied in real world software projects. The density, log level (what to log?) and context (where to log?) are measured for our study. Results: Based on the evidence we collected in 28 top open source projects, we find the logging practice is adopted highly inconsistently among different developers both across projects and even within one project in terms of the density and log levels of logging statements. However, the choice of what context the logging statements to place is consistent to a fair degree. Conclusion: Both the inconsistency in density and log level and the convergence of context have forced us to question whether it is a reliable means to understand the runtime behavior of software systems via analyzing the logs produced by the current logging practice.},
author = {Rong, Guoping and Gu, Shenghui and Zhang, He and Shao, Dong and Liu, Wanggen},
doi = {10.1109/ASWEC.2018.00031},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/How is logging practice implemented in open source software projects A preliminary exploration{\_}Rong et al.{\_}2018.pdf:pdf},
isbn = {9781728112411},
journal = {Proceedings - 25th Australasian Software Engineering Conference, ASWEC 2018},
keywords = {Empirical study,Java-based,Log,Logging practice},
pages = {171--180},
publisher = {IEEE},
title = {{How is logging practice implemented in open source software projects? A preliminary exploration}},
year = {2018}
}
@article{Shahid2016,
abstract = {Change impact is an important issue in software maintenance phase. As retesting is required over a software change, there is a need to keep track of software impact associated with changes. Even a small software change can ripple through to cause a large unintended impact elsewhere in the system that makes it difficult to identify the affected functionalities. The impact after changes demands for a special traceability approach. This paper presents a new approach and prototype tool, Hybrid Coverage Analysis Tool (HYCAT), as a proof of concept to support the software manager or maintainers to manage impact analysis and its related traceability before and after a change in any software artifact. The proposed approach was then evaluated using a case study, On-Board Automobile (OBA), and experimentation. The traceability output before and after changes were produced and analyzed to capture impact analysis. The results of the evaluation show that the proposed approach has achieved some promising output and remarkable understanding as compared to existing approaches.},
author = {Shahid, Muhammad and Ibrahim, Suhaimi},
doi = {10.1109/IBCAST.2016.7429908},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Change impact analysis with a software traceability approach to support software maintenance{\_}Shahid, Ibrahim{\_}2016.pdf:pdf},
isbn = {9781467391276},
journal = {Proceedings of 2016 13th International Bhurban Conference on Applied Sciences and Technology, IBCAST 2016},
keywords = {impact analysis,software change,software maintenance,software traceability},
pages = {391--396},
title = {{Change impact analysis with a software traceability approach to support software maintenance}},
year = {2016}
}
@article{Kherbouche2017,
abstract = {It is widely observed that the poor event logs quality poses a significant challenge to the process mining project both in terms of choice of process mining algorithms and in terms of the quality of the discovered process model. Therefore, it is important to control the quality of event logs prior to conducting a process mining analysis. In this paper, we propose a qualitative model which aims to assess the quality of event logs before applying process mining algorithms. Our ultimate goal is to give process mining practitioners an overview of the quality of event logs which can help to indicate whether the event log quality is good enough to proceed to process mining and in this case, to suggest both the needed preprocessing steps and the process mining algorithm that is most tailored under such a circumstance. The qualitative model has been evaluated using both artificial and real-life case studies.},
author = {Kherbouche, Mohammed Oussama and Laga, Nassim and Masse, Pierre Aymeric},
doi = {10.1109/SSCI.2016.7849946},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Towards a better assessment of event logs quality{\_}Kherbouche, Laga, Masse{\_}2017.pdf:pdf},
isbn = {9781509042401},
journal = {2016 IEEE Symposium Series on Computational Intelligence, SSCI 2016},
keywords = {event logs,process mining,process mining algorithms,qualitative model},
publisher = {IEEE},
title = {{Towards a better assessment of event logs quality}},
year = {2017}
}
@article{Ogheneovo2014,
abstract = {As software becomes more and more complex due to increased number of module size, procedure size, and branching complexity, software maintenance costs are often on the increase. Consider a software such as Windows 2000 operating systems with over 29 million lines of code (LOC), 480,000 pages if printed, a stack of paper 161 feet high, estimate of 63,000 bugs in the software when it was first released [1] and with over 1000 developers, there is no doubt that such a large and com-plex software will require large amount of money (in US Dollars), social and environmental factors to maintain it. It has been estimated that over 70{\%} of the total costs of software development process is expended on maintenance after the software has been delivered. This paper studies the relationship between software complexity and maintenance cost, the factors responsible for soft-ware complexity and why maintenance costs increase with software complexity. Some data col-lected on Windows, Debian Linux, and Linux Kernel operating systems were used. The results of our findings show that there is a strong correlation between software complexity and mainten-ance costs. That is, as lines of code increase, the software becomes more complex and more bugs may be introduced, and hence the cost of maintaining software increases.},
author = {Ogheneovo, Edward E.},
doi = {10.4236/jcc.2014.214001},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/On the Relationship between Software Complexity and Maintenance Costs{\_}Ogheneovo{\_}2014.pdf:pdf},
issn = {2327-5219},
journal = {Journal of Computer and Communications},
keywords = {Software, Software Maintenance, Software Evolution,maintenance costs,software,software evolution,software maintenance},
number = {14},
pages = {1--16},
title = {{On the Relationship between Software Complexity and Maintenance Costs}},
volume = {02},
year = {2014}
}
@article{Cinque2013,
abstract = {Event logs have been widely used over the last three decades to analyze the failure behavior of a variety of systems. Nevertheless, the implementation of the logging mechanism lacks a systematic approach and collected logs are often inaccurate at reporting software failures: This is a threat to the validity of log-based failure analysis. This paper analyzes the limitations of current logging mechanisms and proposes a rule-based approach to make logs effective to analyze software failures. The approach leverages artifacts produced at system design time and puts forth a set of rules to formalize the placement of the logging instructions within the source code. The validity of the approach, with respect to traditional logging mechanisms, is shown by means of around 12,500 software fault injection experiments into real-world systems. {\textcopyright} 2012 IEEE.},
author = {Cinque, Marcello and Cotroneo, Domenico and Pecchia, Antonio},
doi = {10.1109/TSE.2012.67},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Event logs for the analysis of software failures A rule-based approach{\_}Cinque, Cotroneo, Pecchia{\_}2013.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Event log,error detection,logging mechanism,rule-based logging,software failures},
number = {6},
pages = {806--821},
publisher = {IEEE},
title = {{Event logs for the analysis of software failures: A rule-based approach}},
volume = {39},
year = {2013}
}
@book{Mamone1994,
abstract = {Maintenance, traditionally the last phase of the Software Life Cycle, is usually overlooked, under documented, and never appreciated. Maintenance is usually thought of as "something that's done after the work is done" and is usually delegated to junior programmers. As a result the modified system can many times contain more defects that the original system.This article will describe the Software Maintenance Standard and how it can provide the foundation for better control of Maintenance.},
author = {Mamone, Salvatore},
booktitle = {ACM SIGSOFT Software Engineering Notes},
doi = {10.1145/181610.181623},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/The IEEE standard for software maintenance{\_}Mamone{\_}1994.pdf:pdf},
isbn = {0738104183},
issn = {01635948},
keywords = {345 east 47th street,and electronics engineers,copyright {\'{o}} 1993 by,inc,maintenance,new york,ny 10017-2394,software,software maintenance,the institute of electrical,usa},
number = {1},
pages = {75--76},
title = {{The IEEE standard for software maintenance}},
volume = {19},
year = {1994}
}
@article{Li2018,
abstract = {Logging statements are used to record valuable runtime information about applications. Each logging statement is assigned a log level such that users can disable some verbose log messages while allowing the printing of other important ones. However, prior research finds that developers often have difficulties when determining the appropriate level for their logging statements. In this paper, we propose an approach to help developers determine the appropriate log level when they add a new logging statement. We analyze the development history of four open source projects (Hadoop, Directory Server, Hama, and Qpid), and leverage ordinal regression models to automatically suggest the most appropriate level for each newly-added logging statement. First, we find that our ordinal regression model can accurately suggest the levels of logging statements with an AUC (area under the curve; the higher the better) of 0.75 to 0.81 and a Brier score (the lower the better) of 0.44 to 0.66, which is better than randomly guessing the appropriate log level (with an AUC of 0.50 and a Brier score of 0.80 to 0.83) or naively guessing the log level based on the proportional distribution of each log level (with an AUC of 0.50 and a Brier score of 0.65 to 0.76). Second, we find that the characteristics of the containing block of a newly-added logging statement, the existing logging statements in the containing source code file, and the content of the newly-added logging statement play important roles in determining the appropriate log level for that logging statement.},
author = {Li, Heng and Shang, Weiyi and Hassan, Ahmed E.},
doi = {10.1109/SANER.2018.8330234},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Which log level should developers choose for a new logging statement (journal-first abstract){\_}Li, Shang, Hassan{\_}2018.pdf:pdf},
isbn = {9781538649695},
journal = {25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings},
keywords = {log level,ordinal regression model,software logging},
number = {4},
pages = {468},
publisher = {IEEE},
title = {{Which log level should developers choose for a new logging statement? (journal-first abstract)}},
volume = {2018-March},
year = {2018}
}
@article{Dalpiaz2018,
abstract = {90{\%} of agile practitioners employ user stories for capturing requirements. Of these, 70{\%} follow a simple template when creating user stories: As a {\textless}role{\textgreater} I want to {\textless}action{\textgreater}, [so that {\textless}benefit{\textgreater}]. User stories' popularity among practitioners and their simple yet strict structure make them ideal candidates for automatic reasoning based on natural language processing. In our research, we have found that circa 50{\%} of real-world user stories contain easily preventable errors that may endanger their potential. To alleviate this problem, we have created methods, theories and tools that support creating better user stories. This tutorial combines our previous work into a pipeline for working with user stories: (1) The basics of creating user stories, and their use in requirements engineering; (2) How to improve user story quality with the Quality User Story Framework and AQUSA tool; and (3) How to generate conceptual models from user stories using the Visual Narrator and the Interactive Narrator tools. Our toolset is demonstrated with results obtained from 20+ software companies employing user stories.},
author = {Dalpiaz, Fabiano and Brinkkemper, Sjaak},
doi = {10.1109/RE.2018.00075},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Agile requirements engineering with user stories{\_}Dalpiaz, Brinkkemper{\_}2018.pdf:pdf},
isbn = {9781538674185},
journal = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
keywords = {Agile requirements engineering,Natural language processing,User stories},
pages = {506--507},
title = {{Agile requirements engineering with user stories}},
year = {2018}
}
@article{Zhu2019,
abstract = {Logs are imperative in the development and maintenance process of many software systems. They record detailed runtime information that allows developers and support engineers to monitor their systems and dissect anomalous behaviors and errors. The increasing scale and complexity of modern software systems, however, make the volume of logs explodes. In many cases, the traditional way of manual log inspection becomes impractical. Many recent studies, as well as industrial tools, resort to powerful text search and machine learning-based analytics solutions. Due to the unstructured nature of logs, a first crucial step is to parse log messages into structured data for subsequent analysis. In recent years, automated log parsing has been widely studied in both academia and industry, producing a series of log parsers by different techniques. To better understand the characteristics of these log parsers, in this paper, we present a comprehensive evaluation study on automated log parsing and further release the tools and benchmarks for easy reuse. More specifically, we evaluate 13 log parsers on a total of 16 log datasets spanning distributed systems, supercomputers, operating systems, mobile systems, server applications, and standalone software. We report the benchmarking results in terms of accuracy, robustness, and efficiency, which are of practical importance when deploying automated log parsing in production. We also share the success stories and lessons learned in an industrial application at Huawei. We believe that our work could serve as the basis and provide valuable guidance to future research and deployment of automated log parsing.},
author = {Zhu, Jieming and He, Shilin and Liu, Jinyang and He, Pinjia and Xie, Qi and Zheng, Zibin and Lyu, Michael R.},
doi = {10.1109/icse-seip.2019.00021},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Tools and Benchmarks for Automated Log Parsing{\_}Zhu et al.{\_}2019.pdf:pdf},
isbn = {9781728117607},
journal = {2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
pages = {121--130},
publisher = {IEEE},
title = {{Tools and Benchmarks for Automated Log Parsing}},
year = {2019}
}
@article{Slaninova2014,
abstract = {This paper is focused on log files where one log file attribute is an originator of the recorded activity (originator is a person in our case). Hence, based on the similar attributes of people, we are able to construct models which explain certain aspects of a persons behaviour. Moreover, we can extract user profiles based on behaviour and find latent ties between users and between different user groups with similar behaviours. We accomplish this by our new approach using the methods from log mining, business process analysis, complex networks and graph theory. The paper describes the whole process of the approach from the log file to the user graph. The main focus is on the step called 'The finding of user behavioural patterns'.},
author = {Slaninov{\'{a}}, Kateřina},
doi = {10.1109/ISDA.2013.6920751},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/User behavioural patterns and reduced user profiles extracted from log files{\_}Slaninov{\'{a}}{\_}2014.pdf:pdf},
isbn = {9781479935161},
issn = {21647151},
journal = {International Conference on Intelligent Systems Design and Applications, ISDA},
keywords = {analysis of users' behaviour,behavioural patterns,complex networks,user profiles},
pages = {289--294},
publisher = {IEEE},
title = {{User behavioural patterns and reduced user profiles extracted from log files}},
year = {2014}
}
@inproceedings{Ferreira2009,
abstract = {Existing process mining techniques are able to discover process models from event logs where each event is known to have been produced by a given process instance. In this paper we remove this restriction and address the problem of discovering the process model when the event log is provided as an unlabelled stream of events. Using a probabilistic approach, it is possible to estimate the model by means of an iterative Expectaction-Maximization procedure. The same procedure can be used to find the case id. in unlabelled event logs. A series of experiments show how the proposed technique performs under varying conditions and in the presence of certain workflow patterns. Results are presented for a running example based on a technical support process.},
author = {Ferreira, Diogo R. and Gillblad, Daniel},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-03848-8_11},
isbn = {3642038476},
issn = {03029743},
pages = {143--158},
title = {{Discovering process models from unlabelled event logs}},
volume = {5701 LNCS},
year = {2009}
}
@article{Tian2017,
abstract = {A web service reliability test method for C/S architecture software based on log analysis is presented in this paper. In this method, the software usage model is constructed automatically to describe the real situation on the users' access to the web service by Markov chain. The test cases are generated according to Random Walk and applied to software reliability test. In the experiment process, MTBF (focusing on server crash) was chosen to be the software reliability evaluation index. Through the testing and analysis of a real web software, MTBF obtained by testing result is similar to that from the realistic log, and the web service reliability test method is validated.},
author = {Tian, Xuetao and Li, Honghui and Liu, Feng},
doi = {10.1109/QRS-C.2017.38},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Web Service Reliability Test Method Based on Log Analysis{\_}Tian, Li, Liu{\_}2017.pdf:pdf},
isbn = {9781538620724},
journal = {Proceedings - 2017 IEEE International Conference on Software Quality, Reliability and Security Companion, QRS-C 2017},
keywords = {Log Analysis,Markov Usage Model,Reliability Test,Test Cases,Web Service},
pages = {195--199},
publisher = {IEEE},
title = {{Web Service Reliability Test Method Based on Log Analysis}},
year = {2017}
}
@article{Levin2019,
abstract = {Lehman's Laws teach us that a software system will become progressively less satisfying to its users over time, unless it is continually adapted to meet new needs. A line of previous works sought to better understand software maintenance by studying how commits can be classified into three main software maintenance activities. Corrective: fault fixing; Perfective: system improvements; Adaptive: new feature introduction. In this work we suggest visualizations for exploring software maintenance activities in both project and individual developer scopes. We demonstrate our approach using a prototype we have built using the Shiny R framework. In addition, we have also published our prototype as an online demo. This demo allows users to explore the maintenance activities of a number of popular open source projects. We believe that the visualizations we provide can assist practitioners in monitoring and maintaining the health of software projects. In particular, they can be useful for identifying general imbalances, peaks, deeps and other anomalies in projects' and developers' maintenance activities.},
author = {Levin, Stanislav and Yehudai, Amiram},
doi = {10.1109/VISSOFT.2019.00021},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Visually exploring software maintenance activities{\_}Levin, Yehudai{\_}2019.pdf:pdf},
isbn = {9781728149394},
journal = {Proceedings - 7th IEEE Working Conference on Software Visualization, VISSOFT 2019},
keywords = {Predictive Modeling,Software Evolution,Software Maintenance},
pages = {110--114},
publisher = {IEEE},
title = {{Visually exploring software maintenance activities}},
year = {2019}
}
@article{Garlan1999,
abstract = {Over the past decade software architecture has received increasing attention as an important subfield of software engineering. During that time there has been considerable progress in developing the technological and methodological base for treating architectural design as an engineering discipline. However, much remains to be done to achieve that goal. Moreover, the changing face of technology raises a number of new challenges for software architecture. This paper examines some of the important trends of software architecture in research and practice, and speculates on the important emerging trends, challenges, and aspirations.},
author = {Garlan, David},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Software Architecture a Roadmap David Garlan{\_}Garlan{\_}1999.pdf:pdf},
isbn = {1581132530},
journal = {Design},
keywords = {software,software architecture,software design},
title = {{Software Architecture: a Roadmap David Garlan}},
year = {1999}
}
@article{Ping2010,
abstract = {Software maintainability is one important aspect in the evaluation of software evolution of a software product. Due to the complexity of tracking maintenance behaviors, it is difficult to accurately predict the cost and risk of maintenance after delivery of software products. In an attempt to address this issue quantitatively, software maintainability is viewed as an inevitable evolution process driven by maintenance behaviors, given a health index at the time when a software product are delivered. A Hidden Markov Model (HMM) is used to simulate the maintenance behaviors shown as their possible occurrence probabilities. And software metrics is the measurement of the quality of a software product and its measurement results of a product being delivered are combined to form the health index of the product. The health index works as a weight on the process of maintenance behavior over time. When the occurrence probabilities of maintenance behaviors reach certain number which is reckoned as the indication of the deterioration status of a software product, the product can be regarded as being obsolete. Longer the time, better the maintainability would be. {\textcopyright} 2010 IEEE.},
author = {Ping, Liang},
doi = {10.1109/IFITA.2010.294},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/A quantitative approach to software maintainability prediction{\_}Ping{\_}2010.pdf:pdf},
isbn = {9780769541150},
journal = {Proceedings - 2010 International Forum on Information Technology and Applications, IFITA 2010},
keywords = {Hidden markov model,Software maintainability,Software metrics},
pages = {105--108},
publisher = {IEEE},
title = {{A quantitative approach to software maintainability prediction}},
volume = {1},
year = {2010}
}
@article{Kumar2017,
author = {Kumar, Manoj},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Web Log Expert Tool{\_}Kumar{\_}2017.pdf:pdf},
isbn = {9781509056866},
keywords = {web server log,web usage mining},
pages = {296--301},
title = {{Web Log Expert Tool}},
year = {2017}
}
@article{Krol2008,
author = {Krol, D and Scigajlo, M and Trawi{\'{n}}ski, Bogda},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/I Vestigatio of I Ter Et System User Behaviour Usi G Cluster a Alysis{\_}Krol, Scigajlo, Trawi{\'{n}}ski{\_}2008.pdf:pdf},
isbn = {9781424420964},
journal = {Machine Learning},
keywords = {clustering,hcm,server log,user activity,web system},
number = {July},
pages = {12--15},
title = {{I Vestigatio of I Ter Et System User Behaviour Usi G Cluster a Alysis}},
year = {2008}
}
@article{Pecchia2015,
abstract = {{\textcopyright} 2015 IEEE. Practitioners widely recognize the importance of event logging for a variety of tasks, such as accounting, system measurements and troubleshooting. Nevertheless, in spite of the importance of the tasks based on the logs collected under real workload conditions, event logging lacks systematic design and implementation practices. The implementation of the logging mechanism strongly relies on the human expertise. This paper proposes a measurement study of event logging practices in a critical industrial domain. We assess a software development process at Selex ES, a leading Finmeccanica company in electronic and information solutions for critical systems. Our study combines source code analysis, inspection of around 2.3 millions log entries, and direct feedback from the development team to gain process-wide insights ranging from programming practices, logging objectives and issues impacting log analysis. The findings of our study were extremely valuable to prioritize event logging reengineering tasks at Selex ES.},
author = {Pecchia, Antonio and Cinque, Marcello and Carrozza, Gabriella and Cotroneo, Domenico},
doi = {10.1109/ICSE.2015.145},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Industry Practices and Event Logging Assessment of a Critical Software Development Process{\_}Pecchia et al.{\_}2015.pdf:pdf},
isbn = {9781479919345},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Coding practices,Development process,Event logging,Industry domain,Source code analysis},
pages = {169--178},
publisher = {IEEE},
title = {{Industry Practices and Event Logging: Assessment of a Critical Software Development Process}},
volume = {2},
year = {2015}
}
@article{FrancisThamburaj2017,
abstract = {Software maintenance is the most desired, but most elusive and difficult task in software engineering. The cost of maintenance is as high as 60{\%} to 80{\%} of the total cost of the software. So, plenty of researches are going on in software maintenance. Though, object-oriented paradigm has made it easier, it remains the critical hotspot of research. One way of grappling with the maintenance problem, is to use the complexity metrics. Many studies were made to understand the relationship among complexity metrics, cognition, and maintenance. This paper wrestles with four newly proposed object-oriented cognitive complexity metrics to develop maintenance effort prediction models through various statistical techniques. Empirical study designs are made with hypotheses and experimented. Discussion on results prove the maintenance effort prediction models are more robust, more accurate, and can be employed to estimate the maintenance effort.},
author = {{Francis Thamburaj}, T. and Aloysius, A.},
doi = {10.1109/WCCCT.2016.54},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Models for Maintenance Effort Prediction with Object-Oriented Cognitive Complexity Metrics{\_}Francis Thamburaj, Aloysius{\_}2017.pdf:pdf},
isbn = {9781509055739},
journal = {Proceedings - 2nd World Congress on Computing and Communication Technologies, WCCCT 2017},
keywords = {cognitive complexity metrics,maintenance effort prediction,object-oriented metrics,software maintenance},
pages = {191--194},
title = {{Models for Maintenance Effort Prediction with Object-Oriented Cognitive Complexity Metrics}},
year = {2017}
}
@article{Kumar2013,
abstract = {Purpose - The purpose of this paper is to provide an overview of research and development in the measurement of maintenance performance. It considers the problems of various measuring parameters and comments on the lack of structure in and references for the measurement of maintenance performance. The main focus is to determine how value can be created for organizations by measuring maintenance performance, examining such maintenance strategies as condition-based maintenance, reliability-centred maintenance, e-maintenance, etc. In other words, the objectives are to find frameworks or models that can be used to evaluate different maintenance strategies and determine the value of these frameworks for an organization. Design/methodology/approach - A state-of-the-art literature review has been carried out to answer the following two research questions. First, what approaches and techniques are used for maintenance performance measurement (MPM) and which MPM techniques are optimal for evaluating maintenance strategies? Second, in general, how can MPM create value for organizations and, more specifically, which system of measurement is best for which maintenance strategy? Findings - The body of knowledge on maintenance performance is both quantitatively and qualitatively based. Quantitative approaches include economic and technical ratios, value-based and balanced scorecards, system audits, composite formulations, and statistical and partial maintenance productivity indices. Qualitative approaches include human factors, amongst other aspects. Qualitatively based approaches are adopted because of the inherent limitations of effectively measuring a complex function such as maintenance through quantitative models. Maintenance decision makers often come to the best conclusion using heuristics, backed up by qualitative assessment, supported by quantitative measures. Both maintenance performance perspectives are included in this overview. Originality/value - A comprehensive review of maintenance performance metrics is offered, aiming to give, in a condensed form, an extensive introduction to MPM and a presentation of the state of the art in this field. {\textcopyright} Emerald Group Publishing Limited.},
author = {Kumar, Uday and Galar, Diego and Parida, Aditya and Stenstr{\"{o}}m, Christer and Berges, Luis},
doi = {10.1108/JQME-05-2013-0029},
file = {:C$\backslash$:/Users/Corrie C. Scheepers/Google Drive/Masters/Research/Research Articles/Maintenance performance metrics A state-of-the-art review{\_}Kumar et al.{\_}2013.pdf:pdf},
issn = {13552511},
journal = {Journal of Quality in Maintenance Engineering},
keywords = {Framework,Hierarchy,Indicators,Key performance indicators,Maintenance,Maintenance performance measurement,Metrics,Performance,Performance measurement},
number = {3},
pages = {233--277},
title = {{Maintenance performance metrics: A state-of-the-art review}},
volume = {19},
year = {2013}
}
